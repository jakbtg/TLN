{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 6 - Ghigliottina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo il corpus\n",
    "\n",
    "Il mio corpus sarà composto da:\n",
    "- i titoli di tutte le pagine italiane di Wikipedia, presenti nel file *itwiki-latest-all-titles-in-ns0.gz*\n",
    "- tutti i proverbi italiani presi dalla relativa pagina di Wikiquote\n",
    "- tutti i proverbi toscani presi dalla relativa pagina di Wikiquote\n",
    "- tutti i modi di dire italiani presi dalla relativa pagina di Wikiquote"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ripulisco i file di input\n",
    "\n",
    "Tramite la libreria *nltk* ripulisco tutti i file di input e li salvo in una lista che sarà il mio corpus.  \n",
    "Impiega circa 1 minuto, perchè i file sono enormi, produce un corpus di quasi 3 milioni di righe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize, remove stopwords, lemmatize all the input files\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('italian')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "processed_corpus = []\n",
    "\n",
    "def preprocess_file(path):\n",
    "    preprocessed = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.lower()\n",
    "            line = line.replace('_', ' ')\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            filtered = [w for w in tokens if not w in stop_words]\n",
    "            filtered = [w for w in filtered if len(w) > 1] # Remove single letters\n",
    "            filtered = [w for w in filtered if not w.isnumeric()] # Remove numbers\n",
    "            lemmas = [lemmatizer.lemmatize(w) for w in filtered]\n",
    "            preprocessed.append(lemmas)\n",
    "    return preprocessed\n",
    "\n",
    "# I use this to preprocess proverbs files because I wanto to remove the [Fonte x] part\n",
    "def preprocess_proverbi(path):\n",
    "    preprocessed = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.split('[')[0]\n",
    "            line = line.lower()\n",
    "            line = line.replace('_', ' ')\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            filtered = [w for w in tokens if not w in stop_words]\n",
    "            filtered = [w for w in filtered if len(w) > 1] # Remove single letters\n",
    "            filtered = [w for w in filtered if not w.isnumeric()] # Remove numbers\n",
    "            lemmas = [lemmatizer.lemmatize(w) for w in filtered]\n",
    "            preprocessed.append(lemmas)\n",
    "    return preprocessed\n",
    "\n",
    "titles_path = '../data/corpus-es6/itwiki-20221201-all-titles-in-ns0.txt'\n",
    "modididire_path = '../data/corpus-es6/modi-di-dire.txt'\n",
    "proverbi_path = '../data/corpus-es6/proverbi.txt'\n",
    "proverbi_toscani_path = '../data/corpus-es6/proverbi-toscani.txt'\n",
    "\n",
    "processed_corpus += preprocess_file(titles_path)\n",
    "processed_corpus += preprocess_file(modididire_path)\n",
    "processed_corpus += preprocess_proverbi(proverbi_path)\n",
    "processed_corpus += preprocess_proverbi(proverbi_toscani_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(processed_corpus))\n",
    "# print(type(processed_corpus))\n",
    "# print(processed_corpus[len(processed_corpus)-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "- Date le 5 parole di partenza, per ognuna di esse cerco all'interno del corpus tutte le linee (che corrispondono a un proverbio o modo di dire) che contengono quella parola.  \n",
    "- Salvo tutte queste linee in una nuova lista, da cui rimuovo le 5 parole di partenza.\n",
    "- Salvo in una struttura *Counter* tutte le parole che compaiono nella nuova lista.\n",
    "- La sesta parola sarà quella che compare più frequentemente nella nuova lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for all input words\n",
    "def search_all(words, lines):\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        for word in words:\n",
    "            if word in line:\n",
    "                # Remove the word from the line -- Otherwise most probably it will be the most frequent word\n",
    "                line = [w for w in line if w != word]\n",
    "                result.append(line)\n",
    "    freq_words = Counter()\n",
    "    for line in result:\n",
    "        freq_words.update(line)\n",
    "    return result, freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sixth word is morta\n"
     ]
    }
   ],
   "source": [
    "w1 = ['nemmeno', 'neppure', 'brutto', 'prete', 'natura']\n",
    "r1 = 'scherzo'\n",
    "w2 = ['legge', 'patata', 'sotto', 'sacrificio', 'fantasma']\n",
    "r2 = 'spirito'\n",
    "w3 = ['orario', 'portiere', 'brutto', 'prete', 'natura']\n",
    "r3 = 'scherzo'\n",
    "w4 = ['legge', 'patata', 'sotto', 'sacrificio', 'fantasma']\n",
    "r4 = 'spirito'\n",
    "w5 = ['nemmeno', 'neppure', 'brutto', 'prete', 'natura']\n",
    "r5 = 'scherzo'\n",
    "w6 = ['legge', 'patata', 'sotto', 'sacrificio', 'fantasma']\n",
    "r6 = 'spirito'\n",
    "w7 = ['nemmeno', 'neppure', 'brutto', 'prete', 'natura']\n",
    "r7 = 'scherzo'\n",
    "w8 = ['legge', 'patata', 'sotto', 'sacrificio', 'fantasma']\n",
    "r8 = 'spirito'\n",
    "w9 = ['nemmeno', 'neppure', 'brutto', 'prete', 'natura']\n",
    "r9 = 'scherzo'\n",
    "w10 = ['legge', 'patata', 'sotto', 'sacrificio', 'fantasma']\n",
    "r10 = 'spirito'\n",
    "w11 = ['nemmeno', 'neppure', 'brutto', 'prete', 'natura']\n",
    "r11 = 'scherzo'\n",
    "w12 = ['legge', 'patata', 'sotto', 'sacrificio', 'fantasma']\n",
    "r12 = 'spirito'\n",
    "\n",
    "lines, freq_words = search_all(w1, processed_corpus)\n",
    "print(f'The sixth word is {freq_words.most_common(1)[0][0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
