{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 6 - Ghigliottina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo il corpus\n",
    "\n",
    "Il mio corpus sarà composto da:\n",
    "- i titoli di tutte le pagine italiane di Wikipedia\n",
    "- i proverbi italiani presi dalla relativa pagina di Wikiquote\n",
    "- i proverbi toscani presi dalla relativa pagina di Wikiquote\n",
    "- i modi di dire italiani presi dalla relativa pagina di Wikiquote"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ripulisco i file di input\n",
    "\n",
    "Tramite la libreria *nltk* ripulisco tutti i file di input e li salvo in una lista che sarà il mio corpus.  \n",
    "Impiega circa 1 minuto perchè i file sono molto grossi, produce un corpus di quasi 3 milioni di righe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2811279\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, remove stopwords, lemmatize all the input files\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('italian')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "processed_corpus = []\n",
    "\n",
    "def preprocess_file(path):\n",
    "    preprocessed = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.lower()\n",
    "            line = line.replace('_', ' ')\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            filtered = [w for w in tokens if not w in stop_words]\n",
    "            filtered = [w for w in filtered if len(w) > 2] # Remove single and double letters\n",
    "            filtered = [w for w in filtered if not w.isnumeric()] # Remove numbers\n",
    "            filtered = [w for w in filtered if not any(char.isdigit() for char in w)] # Remove words with numbers\n",
    "            lemmas = [lemmatizer.lemmatize(w) for w in filtered]\n",
    "            preprocessed.append(lemmas)\n",
    "    return preprocessed\n",
    "\n",
    "# I use this to preprocess proverbs files because I wanto to remove the [Fonte x] part\n",
    "def preprocess_proverbi(path):\n",
    "    preprocessed = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.split('[')[0]\n",
    "            line = line.lower()\n",
    "            line = line.replace('_', ' ')\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            filtered = [w for w in tokens if not w in stop_words]\n",
    "            filtered = [w for w in filtered if len(w) > 2] # Remove single and double letters\n",
    "            filtered = [w for w in filtered if not w.isnumeric()] # Remove numbers\n",
    "            filtered = [w for w in filtered if not any(char.isdigit() for char in w)] # Remove words with numbers\n",
    "            lemmas = [lemmatizer.lemmatize(w) for w in filtered]\n",
    "            preprocessed.append(lemmas)\n",
    "    return preprocessed\n",
    "\n",
    "titles_path = '../data/corpus-es6/itwiki-20221201-all-titles-in-ns0.txt'\n",
    "modididire_path = '../data/corpus-es6/modi-di-dire.txt'\n",
    "proverbi_path = '../data/corpus-es6/proverbi.txt'\n",
    "proverbi_toscani_path = '../data/corpus-es6/proverbi-toscani.txt'\n",
    "\n",
    "processed_corpus += preprocess_file(titles_path)\n",
    "processed_corpus += preprocess_file(modididire_path)\n",
    "processed_corpus += preprocess_proverbi(proverbi_path)\n",
    "processed_corpus += preprocess_proverbi(proverbi_toscani_path)\n",
    "\n",
    "print(len(processed_corpus))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "- Date le 5 parole di partenza, per ognuna di esse cerco all'interno del corpus tutte le linee che contengono quella parola.  \n",
    "- Salvo tutte queste linee in una nuova lista, da cui rimuovo le 5 parole di partenza.\n",
    "- Salvo in una struttura *Counter* tutte le parole che compaiono nella nuova lista.\n",
    "- La sesta parola sarà quella che compare più frequentemente nella nuova lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for all input words\n",
    "def search_all(words, lines):\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        for word in words:\n",
    "            if word in line:\n",
    "                # Remove the word from the line -- Otherwise most probably it will be the most frequent word\n",
    "                line = [w for w in line if w != word]\n",
    "                result.append(line)\n",
    "    freq_words = Counter()\n",
    "    for line in result:\n",
    "        freq_words.update(line)\n",
    "    return result, freq_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risultati su qualche esempio\n",
    "\n",
    "Ho trovato i seguenti esempi dal sito *https://www.goccediperle.it/hobby/passatempi/il-gioco-della-parola-misteriosa/*, dotati anche di risultato corretto.  \n",
    "I risultati ottenuti purtroppo non sono quasi mai corretti, però si può notare che spesso la parola proposta ha un chiaro nesso con qualcuna delle 5 parole di partenza.  \n",
    "Inoltre ho notato che ogni qual volta compare la parola *cuore*, il risultato è sempre *sacro*.  \n",
    "Questo mi fa pensare che il corpus sia in un qualche modo sbilanciato, come se la parola *cuore* sia presente in molti modi di dire e proverbi e molto spesso collegata alla parola *sacro*.  \n",
    "Inoltre in qualche risultato compaiono parole inglesi, come *the*, *open* e *theft*. Queste vengono sicuramente dalla parte di corpus relativa all'elenco dei titoli delle pagine di Wikipedia.  \n",
    "Questo dovrebbe contenere solamente pagine italiane, ma forse qualche titolo è in inglese, forse qualche pagina di canzoni o film ad esempio.  \n",
    "Pertanto sicuramente il primo miglioramento possibile sarebbe quello di trovare un corpus migliore, più pulito, più bilanciato e che fornisca più conoscenza comune.  \n",
    "Un'altra cosa che potrebbe migliorare le prestazioni sarebbe di considerare *n* parole più frequenti e non solo la prima, e magari confrontare i contesti in cui appaiono le parole proposte.  \n",
    "Infine potrebbe risultare utile incrociare in qualche modo la ricerca sul corpus con anche ricerca di sensi, frame semantici, concetti, esempi di utilizzo, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le cinque parole sono: ['nemmeno', 'neppure', 'brutto', 'prete', 'natura']\n",
      "Ho trovato: morta\n",
      "La risposta corretta è: scherzo\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['preghiera', 'subito', 'rosso', 'verso', 'buona']\n",
      "Ho trovato: riserva\n",
      "La risposta corretta è: sera\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['orario', 'portiere', 'via', 'film', 'libera']\n",
      "Ho trovato: the\n",
      "La risposta corretta è: uscita\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['strada', 'don', 'frutto', 'taglio', 'cappuccetto']\n",
      "Ho trovato: statale\n",
      "La risposta corretta è: bosco\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['concerto', 'governo', 'fuori', 'quotidiano', 'lavatrice']\n",
      "Ho trovato: orchestra\n",
      "La risposta corretta è: programma\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['scena', 'maestro', 'caldo', 'freddo', 'schiena']\n",
      "Ho trovato: altare\n",
      "La risposta corretta è: brivido\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['mente', 'peggio', 'verdura', 'moda', 'prossimo']\n",
      "Ho trovato: storia\n",
      "La risposta corretta è: passato\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['pari', 'olio', 'guardia', 'biancheria', 'valuta']\n",
      "Ho trovato: costiera\n",
      "La risposta corretta è: cambio\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['doppia', 'cuore', 'dopo', 'iva', 'carte']\n",
      "Ho trovato: sacro\n",
      "La risposta corretta è: partita\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['tonno', 'medicina', 'parto', 'bionda', 'acqua']\n",
      "Ho trovato: sapone\n",
      "La risposta corretta è: naturale\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['cercare', 'acida', 'gioco', 'unica', 'problema']\n",
      "Ho trovato: carte\n",
      "La risposta corretta è: soluzione\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['rubare', 'colpo', 'gelosia', 'delitto', 'muta']\n",
      "Ho trovato: stato\n",
      "La risposta corretta è: scena\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['cuore', 'capodanno', 'testa', 'passato', 'parata']\n",
      "Ho trovato: sacro\n",
      "La risposta corretta è: tuffo\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['scuola', 'computer', 'chimica', 'tasse', 'tombola']\n",
      "Ho trovato: superiore\n",
      "La risposta corretta è: cartella\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['notturno', 'ora', 'monte', 'treno', 'radio']\n",
      "Ho trovato: carlo\n",
      "La risposta corretta è: locale\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['doppio', 'carta', 'soldi', 'pasta', 'regalo']\n",
      "Ho trovato: open\n",
      "La risposta corretta è: pacco\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['ingannare', 'ammazzare', 'vola', 'spazio', 'pioggia']\n",
      "Ho trovato: tempo\n",
      "La risposta corretta è: tempo\n",
      "Risposta corretta!\n",
      "--------------------------------------------------------------------------------\n",
      "Le cinque parole sono: ['tutto', 'nobile', 'cucina', 'auto', 'lettura']\n",
      "Ho trovato: theft\n",
      "La risposta corretta è: gas\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "w1 = ['nemmeno', 'neppure', 'brutto', 'prete', 'natura']\n",
    "r1 = 'scherzo'\n",
    "w2 = ['preghiera', 'subito', 'rosso', 'verso', 'buona']\n",
    "r2 = 'sera'\n",
    "w3 = ['orario', 'portiere', 'via', 'film', 'libera']\n",
    "r3 = 'uscita'\n",
    "w4 = ['strada', 'don', 'frutto', 'taglio', 'cappuccetto']\n",
    "r4 = 'bosco'\n",
    "w5 = ['concerto', 'governo', 'fuori', 'quotidiano', 'lavatrice']\n",
    "r5 = 'programma'\n",
    "w6 = ['scena', 'maestro', 'caldo', 'freddo', 'schiena']\n",
    "r6 = 'brivido'\n",
    "w7 = ['mente', 'peggio', 'verdura', 'moda', 'prossimo']\n",
    "r7 = 'passato'\n",
    "w8 = ['pari', 'olio', 'guardia', 'biancheria', 'valuta']\n",
    "r8 = 'cambio'\n",
    "w9 = ['doppia', 'cuore', 'dopo', 'iva', 'carte']\n",
    "r9 = 'partita'\n",
    "w10 = ['tonno', 'medicina', 'parto', 'bionda', 'acqua']\n",
    "r10 = 'naturale'\n",
    "w11 = ['cercare', 'acida', 'gioco', 'unica', 'problema']\n",
    "r11 = 'soluzione'\n",
    "w12 = ['rubare', 'colpo', 'gelosia', 'delitto', 'muta']\n",
    "r12 = 'scena'\n",
    "w13 = ['cuore', 'capodanno', 'testa', 'passato', 'parata']\n",
    "r13 = 'tuffo'\n",
    "w14 = ['scuola', 'computer', 'chimica', 'tasse', 'tombola']\n",
    "r14 = 'cartella'\n",
    "w15 = ['notturno', 'ora', 'monte', 'treno', 'radio']\n",
    "r15 = 'locale'\n",
    "w16 = ['doppio', 'carta', 'soldi', 'pasta', 'regalo']\n",
    "r16 = 'pacco'\n",
    "w17 = ['ingannare', 'ammazzare', 'vola', 'spazio', 'pioggia']\n",
    "r17 = 'tempo'\n",
    "w18 = ['tutto', 'nobile', 'cucina', 'auto', 'lettura']\n",
    "r18 = 'gas'\n",
    "\n",
    "words = [w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, w14, w15, w16, w17, w18]\n",
    "results = [r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11, r12, r13, r14, r15, r16, r17, r18]\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print(f'Le cinque parole sono: {words[i]}')\n",
    "    found = search_all(words[i], processed_corpus)[1]\n",
    "    sixth = found.most_common(1)[0][0]\n",
    "    print(f'Ho trovato: {sixth}')\n",
    "    print(f'La risposta corretta è: {results[i]}')\n",
    "    if sixth == results[i]:\n",
    "        print('Risposta corretta!')\n",
    "    print('-'*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (v3.9.1:1e5d33e9b9, Dec  7 2020, 12:10:52) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
