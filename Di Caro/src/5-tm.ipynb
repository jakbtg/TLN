{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 5 - Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo 4 documenti di input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try wiki api\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "lebanon_page = wiki_wiki.page('Lebanon')\n",
    "bike_page = wiki_wiki.page('Racing bicycle')\n",
    "labrador_page = wiki_wiki.page('Labrador retriever')\n",
    "indie_page = wiki_wiki.page('Indie rock')\n",
    "\n",
    "lebanon_text = lebanon_page.text\n",
    "bike_text = bike_page.text\n",
    "labrador_text = labrador_page.text\n",
    "indie_text = indie_page.text\n",
    "\n",
    "# PROBABLY NOT NEEDED -- Basta lebanon_text, etc.\n",
    "# Create documents as txt files\n",
    "# with open('../data/docs/lebanon.txt', 'w') as f:\n",
    "#     f.write(lebanon_text)\n",
    "\n",
    "# with open('../data/docs/bike.txt', 'w') as f:\n",
    "#     f.write(bike_text)\n",
    "\n",
    "# with open('../data/docs/labrador.txt', 'w') as f:\n",
    "#     f.write(labrador_text)\n",
    "\n",
    "# with open('../data/docs/indie.txt', 'w') as f:\n",
    "#     f.write(indie_text)\n",
    "\n",
    "docs = [lebanon_text, bike_text, labrador_text, indie_text]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dei documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 3996\n",
      "Number of documents: 4\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, remove stopwords, lemmatize a list of documents\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(docs):\n",
    "    prep = docs.copy()\n",
    "    for i in range(len(docs)):\n",
    "        prep[i] = prep[i].lower()\n",
    "        prep[i] = tokenizer.tokenize(prep[i])\n",
    "        prep[i] = [word for word in prep[i] if word not in stop_words]\n",
    "        prep[i] = [lemmatizer.lemmatize(word) for word in prep[i]]\n",
    "        # Remove single characters\n",
    "        prep[i] = [word for word in prep[i] if len(word) > 1]\n",
    "        # Remove word if is a number\n",
    "        prep[i] = [word for word in prep[i] if not word.isnumeric()]\n",
    "    return prep\n",
    "\n",
    "preprocessed_docs = preprocess(docs)\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = Dictionary(preprocessed_docs)\n",
    "\n",
    "# Filter out words that occur more than 50% of the documents\n",
    "# dictionary.filter_extremes(no_above=0.5)\n",
    "\n",
    "# Create corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alleno il modello LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot compute LDA over an empty collection (no terms)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m40\u001b[39m\n\u001b[1;32m      4\u001b[0m iterations \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m----> 6\u001b[0m lda_model \u001b[39m=\u001b[39m LdaModel(corpus\u001b[39m=\u001b[39;49mcorpus, id2word\u001b[39m=\u001b[39;49mdictionary\u001b[39m.\u001b[39;49mid2token, num_topics\u001b[39m=\u001b[39;49mn_topics, passes\u001b[39m=\u001b[39;49mepochs, iterations\u001b[39m=\u001b[39;49miterations)\n\u001b[1;32m      8\u001b[0m lda_model\u001b[39m.\u001b[39mprint_topics()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gensim/models/ldamodel.py:447\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_terms \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_terms \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 447\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot compute LDA over an empty collection (no terms)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    449\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(distributed)\n\u001b[1;32m    450\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_topics \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(num_topics)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot compute LDA over an empty collection (no terms)"
     ]
    }
   ],
   "source": [
    "# Train LDA model\n",
    "n_topics = 4\n",
    "epochs = 40\n",
    "iterations = 1000\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary.id2token, num_topics=n_topics, passes=epochs, iterations=iterations)\n",
    "\n",
    "lda_model.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
