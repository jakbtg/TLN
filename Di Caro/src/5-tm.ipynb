{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 5 - Topic Modelling\n",
    "\n",
    "L'obiettivo è trovare i topic principali di un corpus di testi.  \n",
    "Come corpus utilizzo i testi di 4 pagine di Wikipedia relative agli stessi 4 argomenti trattati nell'esercizio 4:\n",
    "- Lebanon\n",
    "- Racing bicycle\n",
    "- Labrador retriever\n",
    "- Indie rock\n",
    "\n",
    "Questi testi li unisco in un unica lista che, dopo la fase di preprocessing, sarà il corpus su cui allenare il modello scelto.  \n",
    "Il modello scelto è **LDA** (Latent Dirichlet Allocation), fornito dalla libreria *Gensim*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo i testi di riferimento\n",
    "\n",
    "Scarico da Wikipedia i testi delle 4 pagine e li unisco in una lista. Ogni elemento della lista è una stringa che rappresenta il testo di una pagina Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try wiki api\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "lebanon_page = wiki_wiki.page('Lebanon')\n",
    "bike_page = wiki_wiki.page('Racing bicycle')\n",
    "labrador_page = wiki_wiki.page('Labrador retriever')\n",
    "indie_page = wiki_wiki.page('Indie rock')\n",
    "\n",
    "lebanon_text = lebanon_page.text\n",
    "bike_text = bike_page.text\n",
    "labrador_text = labrador_page.text\n",
    "indie_text = indie_page.text\n",
    "\n",
    "docs = [lebanon_text, bike_text, labrador_text, indie_text]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dei documenti\n",
    "\n",
    "Tramite la libreria *nltk* eseguo il preprocessing dei documenti.  \n",
    "Dopodichè con la funzione `gensim.corpora.Dictionary()` creo un dizionario che associa ad ogni parola un indice.  \n",
    "Da questo dizionario rimuovo le parole che appaiono in più del 50% dei documenti.  \n",
    "Infine costruisco una lista di documenti, dove ogni documento è rappresentato come una bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 3792\n",
      "Number of documents: 4\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, remove stopwords, lemmatize a list of documents\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(docs):\n",
    "    prep = docs.copy()\n",
    "    for i in range(len(docs)):\n",
    "        prep[i] = prep[i].lower()\n",
    "        prep[i] = tokenizer.tokenize(prep[i])\n",
    "        prep[i] = [word for word in prep[i] if word not in stop_words]\n",
    "        prep[i] = [lemmatizer.lemmatize(word) for word in prep[i]]\n",
    "        # Remove single characters\n",
    "        prep[i] = [word for word in prep[i] if len(word) > 1]\n",
    "        # Remove word if is a number\n",
    "        prep[i] = [word for word in prep[i] if not word.isnumeric()]\n",
    "    return prep\n",
    "\n",
    "preprocessed_docs = preprocess(docs)\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = Dictionary(preprocessed_docs)\n",
    "\n",
    "# Filter out words that occur more than 50% of the documents \n",
    "# -- no_below=1 means that we keep words that appear at least in one document --> meaning all words in the dictionary (because we have only 4 documents)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
    "\n",
    "# Create corpus as a bag of words representation of the documents\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alleno il modello LDA\n",
    "\n",
    "Con la funzione `gensim.models.ldamodel.LdaModel()` alleno il modello LDA con i documenti preprocessati.  \n",
    "Il modello viene allenato con 4 topic per vedere se riesce a trovare gli argomenti principali dei documenti.  \n",
    "Giocando con *passes* e *iterations* si può migliorare la qualità del modello:\n",
    "- *passes*: numero di passaggi per sul corpus\n",
    "- *iterations*: numero di iterazioni per ogni documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.027*\"labrador\" + 0.025*\"dog\" + 0.011*\"retriever\" + 0.010*\"breed\" + 0.006*\"newfoundland\" + 0.005*\"black\" + 0.005*\"coat\" + 0.005*\"first\" + 0.005*\"bred\" + 0.004*\"earl\"'),\n",
       " (1,\n",
       "  '0.026*\"indie\" + 0.025*\"rock\" + 0.024*\"band\" + 0.012*\"music\" + 0.009*\"pop\" + 0.009*\"scene\" + 0.008*\"term\" + 0.008*\"punk\" + 0.007*\"mainstream\" + 0.007*\"like\"'),\n",
       " (2,\n",
       "  '0.040*\"lebanon\" + 0.017*\"lebanese\" + 0.006*\"war\" + 0.006*\"christian\" + 0.005*\"government\" + 0.005*\"beirut\" + 0.005*\"arab\" + 0.005*\"muslim\" + 0.004*\"mount\" + 0.004*\"al\"'),\n",
       " (3,\n",
       "  '0.028*\"bicycle\" + 0.019*\"racing\" + 0.014*\"road\" + 0.010*\"frame\" + 0.010*\"wheel\" + 0.010*\"bike\" + 0.009*\"cm\" + 0.008*\"tire\" + 0.006*\"must\" + 0.006*\"uci\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train LDA model\n",
    "n_topics = 4\n",
    "epochs = 40\n",
    "iterations = 3000\n",
    "\n",
    "tmp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary.id2token, num_topics=n_topics, passes=epochs, iterations=iterations)\n",
    "\n",
    "lda_model.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
