{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 1 - Definizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leggi il file di input\n",
    "\n",
    "Salvo le varie definizioni degli studenti in un dizionario con:\n",
    "- chiave: parola (tra Emotion, Person, Revenge, Brick)\n",
    "- valore: lista di definizioni date dagli studenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/definizioni.xlsx')\n",
    "students = df.columns[1:].tolist()\n",
    "defs = df[df.columns[0]].tolist()\n",
    "def_dict = {}\n",
    "for definition in defs:\n",
    "    def_dict[definition] = []\n",
    "for student in students:\n",
    "    for definition in defs:\n",
    "        def_dict[definition].append(df[student][df[df.columns[0]] == definition].values[0])\n",
    "\n",
    "# def_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ripulisci il dictionary delle definizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and stopwords, tokenize and lemmatize definitions\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# In tokens I will have a list of lists, where each list is a list of processed tokens for a word\n",
    "tokens = []\n",
    "\n",
    "for key in def_dict:\n",
    "    definitions = [d for d in def_dict[key] if d == d]\n",
    "    tokens.append([lemmatizer.lemmatize(token.lower()) for definition in definitions for token in tokenizer.tokenize(definition) if token.lower() not in stopwords])\n",
    "\n",
    "# for i in range(len(defs)):\n",
    "#     print(defs[i])\n",
    "#     print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcola la similarità tra le definizioni\n",
    "\n",
    "Per ogni parola, calcolo la similarità tra le definizioni date dagli studenti come la sovrapposizione di parole tra le definizioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12176593369253919, 0.05504587155963303, 0.023952095808383235]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcolo la similarità tra le definizioni come la sovrapposizione di token tra le definizioni per ogni parola\n",
    "def similarity(def1, def2):\n",
    "    return len(set(def1).intersection(set(def2))) / min(len(def1), len(def2))\n",
    "\n",
    "sim_matrix = []\n",
    "for i in range(len(defs) - 1):\n",
    "    score = 0\n",
    "    for j in range(i + 1, len(defs)):\n",
    "        score += similarity(tokens[i], tokens[j])\n",
    "    sim_matrix.append(score / (len(defs) - i - 1))\n",
    "\n",
    "# # In sim_matrix I will have a matrix of similarities between definitions\n",
    "# sim_matrix = []\n",
    "# for i in range(len(defs)):\n",
    "#     sim_matrix.append([])\n",
    "#     for j in range(len(defs)):\n",
    "#         sim_matrix[i].append(similarity(tokens[i], tokens[j]))\n",
    "\n",
    "sim_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5587bf738ccec4bb524b17f914a1c07eb93a384af8fabdf9504251bc61fb6171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
