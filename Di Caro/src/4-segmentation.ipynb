{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 4 - Segmentation\n",
    "\n",
    "- Implementare un semplice algoritmo di text segmentation\n",
    "- Usare come test un input di k paragrafi presi da differenti temi (ad es. pagine Wikipedia)  \n",
    "- Il vostro sistema è in grado di trovare i giusti “tagli”?\n",
    "\n",
    "## Idea:\n",
    "\n",
    "- Ripulisco il file di testo da stopwords e punteggiatura e lo tokenizzo e lemmatizzo, ottenendo una lista di relevant words\n",
    "- Suddivido il file di input in una lista di liste: ogni riga del file di input diventa una lista di relevant words\n",
    "- Calcolo la cosine similarity tra ogni riga del file di input e la riga successiva\n",
    "- Posiziono i tagli nei punti di minimo della cosine similarity\n",
    "\n",
    "\n",
    "## File di input\n",
    "\n",
    "Come file di input uso un file di testo contenente un po' di paragrafi presi da Wikipedia riguardo a 4 argomenti diversi:\n",
    "- Lebanon\n",
    "- Racing bike\n",
    "- Labrador retriever\n",
    "- Indie rock  \n",
    "\n",
    "I tagli che dovrebbe trovare il mio algoritmo sono alla linea 27-28, alla linea 58-59 e alla linea 97-98.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing del file di input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('../data/wiki.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(line.strip())\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')\n",
    "preprocessed_data = []\n",
    "for line in data:\n",
    "    words = [lemmatizer.lemmatize(token.lower()) for token in tokenizer.tokenize(line) if token.lower() not in stop_words] \n",
    "    preprocessed_data.append(words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creo una lista di tutte le relevant words del file di input, rimuovendo i duplicati ma mantenendo l'ordine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5115\n",
      "2160\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all words with duplicates\n",
    "all_words = []\n",
    "for line in preprocessed_data:\n",
    "    for word in line:\n",
    "        all_words.append(word)\n",
    "\n",
    "# Remove duplicates but keep the order\n",
    "all_words = list(dict.fromkeys(all_words))\n",
    "\n",
    "# Convert preprocessed data to a list\n",
    "doc = []\n",
    "for line in preprocessed_data:\n",
    "    for word in line:\n",
    "        doc.append(word)\n",
    "\n",
    "print(len(doc))\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 28, 57, 97, 127], 981)\n"
     ]
    }
   ],
   "source": [
    "# Get the segments given a start and end point\n",
    "def get_segments(document, start, end):\n",
    "    segment = []\n",
    "    for i in range(start, end):\n",
    "        segment.append(document[i])\n",
    "    return segment\n",
    "\n",
    "# Get words given a segment\n",
    "def get_words(segment):\n",
    "    words = []\n",
    "    for line in segment:\n",
    "        for word in line:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "# Get score as the sum of the number of times n most frequent words appear in a segment\n",
    "def get_score(segment, n):\n",
    "    words = []\n",
    "    for word in segment:\n",
    "        words.append(word)\n",
    "    c = Counter(words)\n",
    "    score = 0\n",
    "    for word, count in c.most_common(n):\n",
    "        score += count\n",
    "    return score, c.most_common(n)\n",
    "\n",
    "# Get random cut points given the number of lines in the document and the number of segments\n",
    "def get_random_cut_points(n_lines, n_segments):\n",
    "    cut_points = [0,0,0,0]\n",
    "    for i in range(1, n_segments): # 1 has to be 0\n",
    "        cut_points[i] = np.random.randint(1, n_lines - 1)\n",
    "    cut_points.append(n_lines)\n",
    "    return sorted(cut_points)\n",
    "\n",
    "# Run for n iterations and return the best cut points\n",
    "def get_best_cut_points(n_iterations, n_segments, n_lines, n_words, document):\n",
    "    segment_scores = [0,0,0,0]\n",
    "    total_segment_score = 0\n",
    "    best_score = 0\n",
    "    best_cut_points = []\n",
    "    for i in range(n_iterations):\n",
    "        cut_points = get_random_cut_points(n_lines, n_segments)\n",
    "        for i in range(len(cut_points) - 1):\n",
    "            segment = get_segments(document, cut_points[i], cut_points[i+1])\n",
    "            words = get_words(segment)\n",
    "            segment_scores[i] = get_score(words, n_words)[0]\n",
    "        total_segment_score = sum(segment_scores)\n",
    "        if total_segment_score > best_score:\n",
    "            best_score = total_segment_score\n",
    "            best_cut_points = cut_points\n",
    "        \n",
    "    return best_cut_points, best_score\n",
    "\n",
    "# try different values of n_words\n",
    "n_words = 15\n",
    "n_segments = 4\n",
    "n_lines = len(preprocessed_data)\n",
    "cut_points = get_best_cut_points(10000, n_segments, n_lines, n_words, preprocessed_data)\n",
    "print(cut_points)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creo un dizionario per ogni paragrafo del file di input, dove la chiave è la parola e il valore la sua frequenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document (which is a list) by a given number of words\n",
    "# Return a list of lists which is gonna be the list of paragraphs\n",
    "def split_doc(document, n):\n",
    "    return [document[i:i+n] for i in range(0, len(document), n)]\n",
    "\n",
    "# Create a dictionary with keys = all words in a document, values = word counts in a paragraph\n",
    "def paragraph_dict(paragraph, document):\n",
    "    paragraph_dict = {}\n",
    "    for word in document:\n",
    "        paragraph_dict[word] = 0\n",
    "    for word in paragraph:\n",
    "        paragraph_dict[word] += 1\n",
    "    return paragraph_dict\n",
    "\n",
    "# From the dictionary create a list of word counts\n",
    "def paragraph_list(paragraph_dict, document):\n",
    "    paragraph_list = []\n",
    "    for word in document:\n",
    "        paragraph_list.append(paragraph_dict[word])\n",
    "    return paragraph_list\n",
    "\n",
    "# Get for all paragraphs (splitted by n words) a list of word counts\n",
    "def get_paragraphs_list(document, n):\n",
    "    paragraphs = split_doc(document, n)\n",
    "    paragraphs_list = []\n",
    "    for paragraph in paragraphs:\n",
    "        paragraphs_list.append(paragraph_list(paragraph_dict(paragraph, document), document))\n",
    "    return paragraphs_list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcolo della cosine similarity tra ogni riga del file di input e la riga successiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between all paragraphs\n",
    "def cos_sim(n):\n",
    "    cos_sim_list = []\n",
    "    par_list = get_paragraphs_list(doc, n)\n",
    "    for i in range(0, len(par_list) - 3):\n",
    "        par1 = par_list[i]\n",
    "        par2 = par_list[i + 1]\n",
    "        par3 = par_list[i + 2]\n",
    "        par4 = par_list[i + 3]\n",
    "        p_cos1 = par1 + par2\n",
    "        p_cos2 = par3 + par4\n",
    "        cos_sim_list.append(cosine_similarity([p_cos1], [p_cos2])[0][0])\n",
    "        # cos_sim_list.append(cosine_similarity([par1], [par2])[0][0])\n",
    "        # print(f'Paragraphs {i + 1} and {i + 2} are similar with a cosine similarity of {cos_sim_list[i]}')\n",
    "    return cos_sim_list, par_list\n",
    "\n",
    "# print(cos_sim_list)\n",
    "# cos_sim_list = cos_sim()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posiziono i tagli nei punti di minimo della cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the minimums in the list of cosine similarities\n",
    "def find_mins(list):\n",
    "    list = np.array(list)*-1\n",
    "    mins = find_peaks(list)[0]\n",
    "    return mins\n",
    "\n",
    "# Get the indexes of the cut points\n",
    "def get_cut_indexes(document, n):\n",
    "    cos_sim_list, par_list = cos_sim(n)\n",
    "    min_indexes = find_mins(cos_sim_list)\n",
    "    mins = [cos_sim_list[i] for i in min_indexes]\n",
    "    mins = sorted(mins)[0:3]\n",
    "    cut_indexes = [cos_sim_list.index(min) for min in mins]\n",
    "    cut_indexes = sorted(cut_indexes)\n",
    "    cut_indexes = [i * n for i in cut_indexes]\n",
    "    return cut_indexes\n",
    "\n",
    "# Find cut points and cut the document\n",
    "def cut_doc(document, cut_indexes):\n",
    "    paragraphs = []\n",
    "    p1 = document[0:cut_indexes[0]]\n",
    "    paragraphs.append(p1)\n",
    "    p2 = document[cut_indexes[0]:cut_indexes[1]]\n",
    "    paragraphs.append(p2)\n",
    "    p3 = document[cut_indexes[1]:cut_indexes[2]]\n",
    "    paragraphs.append(p3)\n",
    "    p4 = document[cut_indexes[2]:]\n",
    "    paragraphs.append(p4)\n",
    "    return paragraphs\n",
    "\n",
    "# Save results to an output file\n",
    "def save_to_file(paragraphs, n):\n",
    "    with open('../output/segmentation-result.txt', 'w') as f:\n",
    "        for paragraph in paragraphs:\n",
    "            count = 0\n",
    "            count_N = 0\n",
    "            for word in paragraph:\n",
    "                f.write(word + ' ')\n",
    "                count += 1\n",
    "                count_N += 1\n",
    "                if count == 18:\n",
    "                    f.write('\\n')\n",
    "                    count = 0\n",
    "                if count_N == n:\n",
    "                    f.write('\\nParagraph\\n')\n",
    "                    count_N = 0\n",
    "            f.write('\\n\\n\\n\\n\\n\\n SEGMENTATION \\n\\n')\n",
    "\n",
    "n = 45\n",
    "save_to_file(cut_doc(doc, get_cut_indexes(doc, n)), n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot della cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cosine similarity\n",
    "# fig = plt.figure(figsize=(20, 10))\n",
    "# plt.plot(cos_sim(n)[0])\n",
    "# plt.xlabel('Paragraphs')\n",
    "# plt.ylabel('Cosine similarity')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
