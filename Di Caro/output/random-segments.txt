

SEGMENT
Lebanon, officially the Republic of Lebanon or the Lebanese Republic,[a] is a country in Western Asia. It is located between Syria to the north and east and Israel to the south, while Cyprus lies to its west across the Mediterranean Sea; its location at the crossroads of the Mediterranean Basin and the Arabian hinterland has contributed to its rich history and shaped a cultural identity of religious diversity.[15] It is part of the Levant region of the Middle East. Lebanon is home to roughly six million people and covers an area of 10,452 square kilometres (4,036 sq mi), making it the second smallest country in continental Asia. The official language of the state is Arabic, while French is also formally recognized; the Lebanese dialect of Arabic is used alongside Modern Standard Arabic throughout the country.
The earliest evidence of civilization in Lebanon dates back over 7000 years, predating recorded history.[16] Modern-day Lebanon was home to the Phoenicians, a maritime culture that flourished for almost 3000 years (c. 3200–539 BC). In 64 BC, the Roman Empire conquered the region, and it eventually became among the empire's leading centers of Christianity.[17] The Mount Lebanon range saw the emergence of a monastic tradition known as the Maronite Church. Upon the region's conquest by the early Arab Muslims, the Maronites held onto their religion and identity. However, a new religious group known as the Druze eventually established themselves in Mount Lebanon as well, generating a religious divide that has lasted for centuries. During the Crusades, the Maronites re-established contact with the Roman Catholic Church and asserted their communion with Rome. The Maronite Catholic and the Druze founded modern Lebanon in the early eighteenth century, through the ruling and social system known as the "Maronite-Druze dualism" in Mount Lebanon Mutasarrifate.[18]
Lebanon was conquered by the Ottoman Empire in the 16th century and remained under its rule for the next 400 years. Following the empire's collapse after World War I, the five Ottoman provinces constituting modern-day Lebanon came under the French Mandate for Syria and the Lebanon, under which its French-ruled predecessor state of Greater Lebanon was established. Following the invasion and occupation of the French Third Republic by Nazi Germany during World War II, French rule over the region weakened. Upon gaining its independence from Free France in 1943, Lebanon established a unique confessionalist form of government, with the state's major religious sects being apportioned specific political powers. Lebanon initially was relatively stable.[19] This stability was short-lived and was ultimately shattered by the outbreak of large-scale fighting in the Lebanese Civil War (1975–1990) between various political and sectarian factions. During this period, Lebanon was also subjected to overlapping foreign military occupations by Syria from 1976 to 2005 and by Israel from 1985 to 2000. Since the end of the war, there have been extensive efforts to revive the economy and rebuild national infrastructure.[20]
Lebanon is a developing country, ranking 112th on the Human Development Index.[21] It has been classified as an upper middle income state.[22] However, the Lebanese liquidity crisis, corruption as well as recent events have precipitated the collapse of currency, political instability, widespread shortages, high unemployment and poverty. The World Bank defined the economic crisis in Lebanon as one of the worst in the world since the 19th century.[23][24] Despite the country's small size,[25] Lebanese culture is renowned both in the Middle East and globally, primarily powered by its extensive diaspora. Lebanon is a founding member of the United Nations and is a member of the Arab League, the Non-Aligned Movement, the Organization of Islamic Cooperation, and the Organisation internationale de la Francophonie.
The borders of contemporary Lebanon are a product of the Treaty of Sèvres of 1920. Its territory was in the core of the Bronze Age Canaanite (Phoenician) city-states. As part of the Levant, it was part of numerous succeeding empires throughout ancient history, including the Egyptian, Assyrian, Babylonian, Achaemenid Persian, Hellenistic, Roman and Sasanid Persian empires.
After the 7th-century Muslim conquest of the Levant, it was part of the Rashidun, Umayyad, Abbasid Seljuk and Fatimid empires. The crusader state of the County of Tripoli, founded by Raymond IV of Toulouse in 1102, encompassed most of present-day Lebanon, falling to the Mamluk Sultanate in 1289 and finally to the Ottoman Empire in 1516.[30] With the dissolution of the Ottoman Empire, Greater Lebanon fell under French mandate in 1920,[31] and gained independence under president Bechara El Khoury in 1943. Lebanon's history since independence has been marked by alternating periods of relative political stability and prosperity based on Beirut's position as a regional center for finance and trade, interspersed with political turmoil and armed conflict (1948 Arab–Israeli War, Lebanese Civil War 1975–1990, 2005 Cedar Revolution, 2006 Lebanon War, 2007 Lebanon conflict, 2006–08 Lebanese protests, 2008 conflict in Lebanon, 2011 Syrian Civil War spillover, and 2019–20 Lebanese protests).[32]
Ancient Lebanon
Main article: History of ancient Lebanon
Map of Phoenicia and trade routes
Evidence dating back to an early settlement in Lebanon was found in Byblos, considered among the oldest continuously inhabited cities in the world.[16] The evidence dates back to earlier than 5000 BC. Archaeologists discovered remnants of prehistoric huts with crushed limestone floors, primitive weapons, and burial jars left by the Neolithic and Chalcolithic fishing communities who lived on the shore of the Mediterranean Sea over 7,000 years ago.[33]
Lebanon was part of northern Canaan, and consequently became the homeland of Canaanite descendants, the Phoenicians, a seafaring people who spread across the Mediterranean in the first millennium BC.[34] The most prominent Phoenician cities were Byblos, Sidon and Tyre, while their most famous colonies were Carthage in present-day Tunisia and Cádiz in present-day Spain. The Phoenicians are credited with the invention of the oldest verified alphabet, which subsequently inspired the Greek alphabet and the Latin one thereafter.[35] The cities of Phoenicia were incorporated into the Persian Achaemenid Empire by Cyrus the Great in 539 BCE.[36] The Phoenician city-states were later incorporated into the empire of Alexander the Great following the Siege of Tyre in 332 BC.[36]
In 64 BC, the Roman general Pompey the Great had the region of Syria annexed into the Roman Republic. The region was then split into two Imperial Provinces under the Roman Empire, Coele Syria and Phoenice, the latter which the land of present day Lebanon was apart of.
Medieval Lebanon
The region that is now Lebanon, as with the rest of Syria and much of Anatolia, became a major center of Christianity in the Roman Empire during the early spread of the faith. During the late 4th and early 5th century, a hermit named Maron established a monastic tradition focused on the importance of monotheism and asceticism, near the Mediterranean mountain range known as Mount Lebanon. The monks who followed Maron spread his teachings among Lebanese in the region. These Christians came to be known as Maronites and moved into the mountains to avoid religious persecution by Roman authorities.[37] During the frequent Roman-Persian Wars that lasted for many centuries, the Sassanid Persians occupied what is now Lebanon from 619 till 629.[38]
During the 7th century the Muslim Arabs conquered Syria establishing a new regime to replace the Byzantines. Though Islam and the Arabic language were officially dominant under this new regime, the general populace nonetheless only gradually converted from Christianity and the Syriac language. The Maronite community, in particular, managed to maintain a large degree of autonomy despite the succession of rulers over Lebanon and Syria.
The relative (but not complete) isolation of the Lebanese mountains meant the mountains served as a refuge in the times of religious and political crises in the Levant. As such, the mountains displayed religious diversity and existence of several well established sects and religions, notably, Maronites, Druze, Shiite Muslims, Ismailis, Alawites and Jacobites.
During the 11th century the Druze religion emerged from a branch of Shia Islam. The new religion gained followers in the southern portion of Mount Lebanon. The southern portion of Mount Lebanon was ruled by Druze feudal families to the early 14th century. The Maronite population increased gradually in Northern Mount Lebanon and the Druze have remained in Southern Mount Lebanon until the modern era. Keserwan, Jabal Amel and the Beqaa Valley was ruled by Shia feudal families under the Mamluks and the Ottoman Empire. Major cities on the coast, Sidon, Tyre, Acre, Tripoli, Beirut, and others, were directly administered by the Muslim Caliphs and the people became more fully absorbed by the Arab culture.
Following the fall of Roman Anatolia to the Muslim Turks, the Byzantines put out a call to the Pope in Rome for assistance in the 11th century. The result was a series of wars known as the Crusades launched by the Franks from Western Europe to reclaim the former Byzantine Christian territories in the Eastern Mediterranean, especially Syria and Palestine (the Levant). The First Crusade succeeded in temporarily establishing the Kingdom of Jerusalem and the County of Tripoli as Roman Catholic Christian states along the coast.[44] These crusader states made a lasting impact on the region, though their control was limited, and the region returned to full Muslim control after two centuries following the conquest by the Mamluks.
Among the most lasting effects of the Crusades in this region was the contact between the Franks (i.e., the French) and the Maronites. Unlike most other Christian communities in the Eastern Mediterranean, who swore allegiance to Constantinople or other local patriarchs, the Maronites proclaimed allegiance to the Pope in Rome. As such the Franks saw them as Roman Catholic brethren. These initial contacts led to centuries of support for the Maronites from France and Italy, even after the fall of the Crusader states in the region.
Ottoman Lebanon
See also: Emirate of Mount Lebanon, Sidon Eyalet, and Mount Lebanon Mutasarrifate
During this period Lebanon was divided into several provinces: Northern and Southern Mount Lebanon, Tripoli, Baalbek and Beqaa Valley, and Jabal Amel.
Fakhreddine II Palace, 17th century
In southern Mount Lebanon in 1590, Fakhr-al-Din II became the successor to Korkmaz. He soon established his authority as paramount prince of the Druze in the Shouf area of Mount Lebanon. Eventually, Fakhr-al-Din II was appointed Sanjakbey (Governor) of several Ottoman sub-provinces, with responsibility for tax-gathering. He extended his control over a substantial part of Mount Lebanon and its coastal area, even building a fort as far inland as Palmyra.[45] This over-reaching eventually became too much for Ottoman Sultan Murad IV, who sent a punitive expedition to capture him in 1633. He was taken to Istanbul, kept in prison for two years and then executed along with one of his sons in April 1635.[46] Surviving members of Fakhr al-Din's family ruled a reduced area under closer Ottoman control until the end of the 17th century.
1862 map drawn by the French expedition of Beaufort d'Hautpoul,[47] later used as a template for the 1920 borders of Greater Lebanon.[48][49]
On the death of the last Maan emir, various members of the Shihab clan ruled Mount Lebanon until 1830. The relationship between the Druze and Christians in Lebanon has been characterized by harmony and peaceful coexistence,[50][51][52][53] with amicable relations between the two groups prevailing throughout history, with the exception of some periods, including 1860 Mount Lebanon civil war; Approximately 10,000 Christians were killed by the Druzes during inter-communal violence in 1860.[54] Shortly afterwards, the Emirate of Mount Lebanon, which lasted about 400 years, was replaced by the Mount Lebanon Mutasarrifate, as a result of a European-Ottoman treaty called the Règlement Organique. The Mount Lebanon Mutasarrifate[55][56][57] (1861–1918, Arabic: متصرفية جبل لبنان; Turkish: Cebel-i Lübnan Mutasarrıflığı) was one of the Ottoman Empire's subdivisions following the Tanzimat reform. After 1861 there existed an autonomous Mount Lebanon with a Christian mutasarrıf, which had been created as a homeland for the Maronites under European diplomatic pressure following the 1860 massacres. The Maronite Catholics and the Druze founded modern Lebanon in the early eighteenth century, through the ruling and social system known as the "Maronite-Druze dualism" in Mount Lebanon Mutasarrifate.[18] The Baalbek and Beqaa Valley and Jabal Amel was ruled intermittently by various Shia feudal families, especially the Al Ali Alsagheer in Jabal Amel that remained in power until 1865 when Ottomans took direct ruling of the region. Youssef Bey Karam,[58] a Lebanese nationalist played an influential role in Lebanon's independence during this era.
Around 100,000 people in Beirut and Mount Lebanon died of starvation during World War I.[59]


SEGMENT
A racing bicycle, also known as a road bike is a bicycle designed for competitive road cycling, a sport governed by and according to the rules of the Union Cycliste Internationale (UCI).
Racing bicycles are designed for maximum performance while remaining legal under the UCI rules. They are designed to minimise aerodynamic drag, rolling resistance, and weight, and balance the desire for stiffness for pedaling effiency with the need for some flexibility for comfort. Racing bicycles sacrifice comfort for speed compared to non-racing bicycles. The drop handlebars are positioned lower than the saddle in order to put the rider in a more aerodynamic posture. The front and back wheels are close together so the bicycle has quick handling, which is preferred by experienced racing cyclists. The derailleur gear ratios are closely spaced so that the rider can pedal at their optimum cadence. However, racing bicycles must retain the ability to maneuver safely within a tightly-packed peloton, and be sufficiently comfortable to ride for races of six hours or more.
Bicycles and most wheels ridden in professional competition must be type-approved by the UCI,[1] and made available for commercial sale.[2] It is common for professional road cycling teams to use prototype bicycles and equipment before they become commercially available.
Racing bicycles are generally legal for use on public roads and are widely used for non-racing fitness and utility riding.
Distinction between road bicycles and others
Bicycles for racing on velodromes are track bicycles; bicycles for racing off-road are mountain bicycles, cyclo-cross bicycles, gravel bicycles or cycle speedway bicycles; bicycles that race according to the rules of the International Human Powered Vehicle Association include faired recumbent bicycles which, on flat ground, are the fastest bicycles in the world.[3][4][5][6] Recumbents were excluded from the UCI definition of a bicycle on 1 April 1934.
Time trial bicycles are a subset of road bicycles that are designed for time trial events. The UCI rules for these bikes are slightly less prescriptive than those for "massed start road races" (see rules 1.3.020 to 1.3.023).
Triathlon bicycles are governed by International Triathlon Union (ITU) rules, which allow more recent technological developments than do the UCI rules.
Owing to the lower air resistance and better body stance for pushing, road bicycles tend to be more efficient for use on smooth roads. Cyclo-cross bicycles, which are used for racing on off-road circuits, are closer to racing bikes than to mountain bikes. They have wider, treaded tires are less efficient than racing bikes at higher speeds on sealed roads. Gravel bicycles evolved from cyclo-cross bicycles and are drop-bar bicycles designed for riding on maintained but unsealed roads, often over very long distances.
Frame
The frame of a road bicycle must, according to the UCI regulations, be constructed using a "main triangle" with three straight tubular shapes—the top tube, down tube, and seat tube. These three tubes, and other parts of the frame, need not be cylindrical, however, and many racing bicycles feature frames that use alternative shapes. Traditionally, the top tube of a racing bicycle is close to parallel with the ground when the bicycle is in its normal upright position. Some racing bicycles, however, have a top tube that slopes down towards the rear of the bicycle; the "compact" frame geometry was popularized by Giant.[citation needed]
Frame manufacturers are free to use any material they choose in the frame. For most of the history of road racing, bicycle frames were constructed from steel tubing, and aluminium and titanium alloys were also used successfully in racing bicycles. Racing bicycles in these three materials are still commercially available and are still used by some amateur racing cyclists or in vintage racing classes. However, virtually all professional road racing cyclists now use frames constructed from various carbon fiber composite materials,[citation needed] and a typical modern carbon fiber frame weighs less than 1 kg (2.2 lbs).
Particularly since the introduction of carbon fiber frames, the shape of the tubes that make up the frame has increasingly diverged from the traditional cylinder, either to modify the ride characteristics of the bicycle, reduce weight, or simply achieve styling differentiation. However, a recent trend in road bicycle frame design is tubing claimed to reduce aerodynamic drag, adopting many design features from time trial bicycles, and the majority of top-level teams use bikes with aerodynamic tube shaping at least some of the time.
Unlike mountain bikes, most road bicycles do not have a dedicated suspension. The majority of damping is provided by the tires, padding in a cyclist's clothing, seat, and handlebar tape. Wheels, frames, and forks provide a limited amount of flex, and manufacturers tune the thickness and construction of frame tubing to offer the best compromise between a stiff frame (desired for pedalling efficiency, particularly when sprinting) and ride comfort. Additionally, a few manufacturers offer racing bicycles using additional flexing points or vibration-absorbing materials at key points in the frame for additional comfort.[7]
Wheels and tires
Most road bicycles use 700C bicycle wheels (622 mm bead seat diameter), with matching tyres that are 23-28 mm wide. Wider tires became more common over the 2010s, as more sophisticated measurement showed that wider tires increase rider comfort, improve grip, and decrease rolling resistance without unduly affecting aerodynamics.
The wheels greatly affect the performance of a racing bike. The rim of the wheel can be shaped for greater aerodynamic efficiency making a triangular cross-section to form a teardrop with the tire.
For aerodynamics and rotating weight, it is generally better to reduce the number of spokes in the wheel. For high-end wheelsets, the spokes can be shaped to have a bladed cross-section, further reducing wind resistance.
The most common wheel material for professional racing is now molded carbon fiber rims being a popular choice for pro-level racers and enthusiasts. Aluminium rims are still widely used outside racing and by some less well-funded amateur racers. Carbon fiber rims are lighter than the same shape in aluminium, allowing riders to choose "deeper", more aerodynamic rims without an unacceptable weight penalty. Race-grade wheelsets are very expensive and often fragile. Riders who race often choose to own at least two pairs of wheels: a heavier, more durable, and cheaper wheelset for training, and a lighter, more aerodynamic wheelset for racing. Racers with sufficient resources may have multiple racing wheelsets to choose from depending on the course and weather conditions; deeper rims lose their aerodynamic advantage, and are hard to control, in high crosswinds, and on mountainous courses the lightest possible wheelset may be preferred by some riders.
To reduce both air resistance and rolling resistance on the road, tires are lightweight, narrow, and have a thin, smooth tread. Track racing tires can be inflated up to circa 200 pounds per square inch (14 bar). Tires for road racing used to be inflated to pressures almost as high in the belief that this minimised rolling restistance. However, it was discovered that on real road surfaces, much lower tyre pressures or around 70 pounds per square inch (4.8 bar), depending on rider weight and conditions, result in both a more comfortable ride and lower rolling resistance.[8] Until recently, most racing bikes used tubular tires which have no beads: they are sewn around the tube and glued to the rim. These tires provide an advantage in weight (lacking the relatively heavy wire bead), grip and pinch flat protection, but their greatest advantage lies in the ability to use a very lightweight simple box-section rim, rather than the U-shaped clincher rim. A U-shaped clincher rim must be made of relatively heavier gauge material to prevent the tire pressure from spreading the inherently weak U shape and allowing the tire to come off the rim. Advances in tire technology, however, have seen the far more practical (due to greater ease of changeability) clincher (beaded) tire close the gap.[9] Some teams are using tubeless tires, which are claimed to offer the resistance to pinch flats of tubular tires, as well as greater ease of fitment and lower rolling resistance than tubulars.[10]
Wheel moment of inertia is a controversial subject. In this article: wheel theory, the author does some calculations on wheel effects. Moment of inertia changes result in a decrease in watts of between .004 and .022%, while lower mass provided between .2 and .46%, and better aerodynamics provided between .6 and 1.8% decrease in power. Therefore, wheel moment of inertia effects are neither noticeable nor important. At the same time, a product launched in 2008 to dynamically alter the rotating inertia of bicycle wheels claims to have "outperformed the standard, equivalent wheel by 5.6sec/mile."[11]
Components
Road bike components are collectively referred to as the groupset. The quality of the groupset determines how refined the bike feels, how much maintenance it requires, and contributes to the performance of the bike. The three major groupset manufacturers of complete groupsets for racing bicycles are Shimano, SRAM, and Campagnolo. Some companies only produce specific components of the groupset, such as Full Speed Ahead (often abbreviated to FSA). The companies have different design strategies, and some cyclists have great brand loyalty for one or the other.
In the early 1990s, Shimano introduced dual-control with a system called Shimano Total Integration (STI). STI is characterized by its combined brake and shift levers, or "brifters". Previously, the shifters were mounted on the stem, handlebar ends or the down tube of the frame. Dual control addressed the problem of having to reposition a hand to change gears. STI was followed by the competing Campagnolo/Sachs Ergolever. SRAM uses a technology known as Double Tap for their integrated shifter/brake lever.
In the mid 1990s Mavic, known for their wheelsets, introduced an electronic shifting system which was pioneered in the Tour de France by American Greg LeMond and later on by Briton Chris Boardman, who liked the fact that the system allowed him to shift from his aerobars and his brake levers. The system did not catch on due to technological hurdles. In 2009, Shimano commercially released the Dura-Ace Di2 electronic shifter, and Campagnolo and SRAM followed suit in the early 2010s. As of 2022, the vast majority of top-level professional teams use electronic shifting, as do many amateur riders. Mechanical derailleurs remain popular with amateur racers and recreational riders for cost reasons.
Groupset manufacturers have gradually increased the number of gears on racing bicycles, allowing a greater range between the highest and lowest gears while maintaining relatively small gaps between each successive gear. As of 2022, most racing bicycles used in professional racing have 2 front chainrings and 11 or 12 gears on the rear cassette.
Carbon fiber has also become more popular for components. Shimano, Campagnolo and SRAM have introduced carbon fiber for their high-end shifters and brake levers, cranks, and parts of their derailleurs. Carbon fiber stems, handlebars, shoe soles, forks and seatposts are also more commonplace, including integrated stem/handlebar combinations. The advantages of carbon fiber are low weight as well as increased vibration damping leading to a more comfortable ride.
As of 2022, the majority of professional teams use hydraulic disc brakes for most or all races. A few top-level professional teams still use bikes with caliper brakes in mountain stages, as their disc-brake models are significantly heavier than the UCI's legal minimum weight of 6.8kg. Disc brakes offer better stopping performance, particularly in wet conditions. For many years, racing bicycles were required to use caliper brakes by UCI regulation, but manufacturers began producing road bikes with disc brakes for recreational use in the mid-2010s, and their use was trialled in professional racing in 2016 and 2017. Rider disquiet over the crash safety of disc brakes led to the introduction of versions with chamfered edges to reduce the risk of the rotating disc acting as a cutting blade. In 2018, the UCI concluded the trial and legalised the use of disc brakes in all road racing events.
Non-competitive uses
Road bicycles are built for casual recreational use, often labeled as "sportif" bicycles or "dropped-bar fitness bikes". These have much in common with a competitive racing bicycle, but the frame geometry is relaxed to make the bicycle more comfortable over long distances, though less effective for short bursts of speed. They usually have a wider range of gear ratios (with greater gaps between each ratio) and fewer high-tech racing features.[12]
For recreational road cycling, the road-style bicycles with drop handlebars used to be the norm in the 1980s, along with the touring bicycle (a drop-handlebar bike with a slightly longer wheelbase to provide a smoother ride and less sensitivity to steer and balance disturbances). These designs, called "road bikes" or colloquially, "roadies" or "racing bikes", have become less popular in recent years[when?]. The Mountain bike geometry, with its straight handlebars and upright sitting position (resembling the city bike), have moved into the high-performance spaces, and high production volumes have brought down costs significantly.


SEGMENT
The Labrador Retriever or simply Labrador is a British breed of retriever gun dog. It was developed in the United Kingdom from fishing dogs imported from the colony of Newfoundland (now a province of Canada), and was named after the Labrador region of that colony. It is among the most commonly kept dogs in several countries, particularly in the Western world.
The Labrador is friendly, energetic, and playful.[2] It was bred as a sporting and hunting dog but is widely kept as a companion dog. It may also be trained as a guide or assistance dog, or for rescue or therapy work.[3]
In the 1830s, the 10th Earl of Home and his nephews the 5th Duke of Buccleuch and Lord John Scott,[4] imported progenitors of the breed from Newfoundland to Europe for use as gun dogs. Another early advocate of these Newfoundland fishing dogs was the 2nd Earl of Malmesbury, who bred them for their expertise in waterfowling.[4]
During the 1880s, the 3rd Earl of Malmesbury, the 6th Duke of Buccleuch, and the 12th Earl of Home collaborated to develop and establish the Labrador Retriever breed. The dogs Buccleuch Avon and Buccleuch Ned, given by Malmesbury to Buccleuch, were mated with bitches carrying blood from those originally imported by the 5th Duke and the 10th Earl of Home. The offspring are the ancestors of all modern Labradors.
The Labrador breed dates back to at least the 1830s, when St. John's water dogs bred by European settlers in Newfoundland, were first introduced to Britain from ships trading between Canada and Poole in Dorsetshire. These were then bred with British hunting dogs to create what became known as the Labrador Retriever. Its early patrons included the Earl of Malmesbury, the Duke of Buccleuch, the Earl of Home, and Sir John Scott. Early writers have confused the Labrador with the much larger Newfoundland and the Lesser Newfoundland, with Charles St. John even referring to the Lesser Newfoundland as the Newfoundland. Colonel Peter Hawker describes the first Labrador as being not larger than an English Pointer, more often black than other colours, long in its head and nose with a deep chest, fine legs, and short and smooth coat, and did not carry its tail as highly as the Newfoundland.[5] Hawker distinguishes the Newfoundland from both the "proper Labrador" and St. John's breed of these dogs[5][6] in the fifth edition of his book Introductions to Young Sportsman, published in 1846.[6]
The first photograph of the breed was taken in 1857 (the Earl of Home's dog "Nell", described both as a Labrador and a St. John's water dog). By 1870, the name Labrador Retriever became common in England.[7] The first yellow Labrador on record was born in 1899 (Ben of Hyde, kennels of Major C.J. Radclyffe), and the breed was recognised by the Kennel Club in 1903.[8] The first American Kennel Club (AKC) registration was in 1917. The Liver (now typically called Chocolate) Labrador emerged in the late 1800s, with liver-coloured pups documented at the Buccleuch kennels in 1892.
Characteristics
The head is broad with a pronounced stop
Black
Chocolate
There is a great deal of variety among Labradors. The following characteristics are typical of the conformation show bred (bench-bred) lines of this breed in the United States and are based on the American Kennel Club standard.[2] Significant differences between UK and U.S. standards are noted.
Size: Labradors are a medium-large breed. They should be as long from the withers to the base of the tail as they are from the floor to the withers. The AKC standard includes an ideal weight for dogs of 25–36 kg (55–80 lb) and for bitches as 25–32 kg (55–70 lb).[2] The guidelines for height vary between the AKC, which gives 55 to 62 centimetres (21.5 to 24.5 in) for dogs and 55 to 60 centimetres (21.5 to 23.5 in) for bitches,[2] The Kennel Club which advises that dogs should be 56 to 57 centimetres (22 to 22.5 in) with bitches between 55 to 56 centimetres (21.5 to 22 in),[9] and the FCI which quotes a range of 56 to 57 centimetres (22 to 22.5 in) for dogs with bitches ideal at 54 to 56 centimetres (21.5 to 22 in).[10]
Coat: The Labrador Retriever's coat should be short and dense, but not wiry. The coat is water-resistant, so the dog does not get cold when taking to the water in the winter. That means that the dog naturally has a slightly dry, oily coat. Acceptable colours are black, yellow, and chocolate.[11]
Head: The head should be broad with slightly pronounced eyebrows. The eyes should be kind and expressive. Appropriate eye colours are brown and hazel. The lining around the eyes should be black. The ears should hang close to the head and be set slightly above the eyes.
Jaws: The jaws should be strong and powerful. The muzzle should be of medium length and should not be too tapered. The jaws should hang slightly and curve gracefully back.
Body: The body should have a powerful and muscular build.
The tail and coat are designated "distinctive [or distinguishing] features" of the Labrador by both the Kennel Club and AKC.[2][9] The AKC adds that the "true Labrador Retriever temperament is as much a hallmark of the breed as the 'otter' tail."[2]
Colour
Main article: Labrador Retriever coat colour genetics
three Labrador Retrievers: yellow, black and chocolate
The three colour varieties: yellow, black and chocolate
Labradors are registered in three colours: solid black, yellow (considered from creamy white to fox-red) and chocolate (medium to dark brown and originally called "liver"). [12]
Puppies of all colours can occur in the same litter. Coat colour is determined primarily by three genes. The first gene (the B locus) determines the density of the coat's eumelanin pigment granules, if that pigment is allowed: dense granules result in a black coat, and sparse ones give a chocolate coat. The second (E) locus determines whether the eumelanin is produced at all. A dog with the recessive e allele will produce only phaeomelanin pigment and will be yellow regardless of its genotype at the B locus. The genes known previously have had their number increased by the introduction of the K locus, where the dominant "black" allele KB is now known to reside.[13]
According to a 2011 study, 13 out of 245 Labradors studied were heterozygous for the M264V mutation responsible for the melanistic mask, and one was homozygous. Within the breed, this trait is not visible.[14]
The most common places where pigmentation is visible are the nose, lips, gums, and the rims of the eyes
Show and field lines
Head and muzzle appearance: field (left), and show (right), showing the shorter muzzle length, more solid appearance head, and "pronounced" stop of the latter
As a result of specialised breeding, there are significant differences between field and trial-bred and show-bred lines of Labradors. In the United States, the former are sometimes mistakenly referred to as "American" and the latter as "English", in fact, both field and show types are bred in both countries and all Labrador Retrievers are descended from British lines.[
Use
Retrieving game
Labrador Retrievers have proven to have a high success rate at becoming guide dogs. A study was recently done on how well four different breeds (Labrador Retriever, Golden Retriever, Labrador Retriever/Golden Retriever Mix, and German Shepherds) trained to become guide dogs. In this experiment, German Shepherds had the highest chance of not completing it. Labrador Retrievers and Labrador Retriever/Golden Retriever Mix had the highest success rate. However, German Shepherds and Golden Retrievers had a higher success rate after going through longer training than the training required for Labrador Retrievers.[16][17]
Labradors are powerful and indefatigable swimmers noted for their ability to tolerate the coldest of water for extended periods of time. Their ability to work quietly alongside hunters while watching for birds to fall from the sky, marking where they land and then using their outstanding nose to find and retrieve dead or wounded birds has made them the king of waterfowl retrievers.[18]
They can also be used for pointing and flushing, and make excellent upland game hunting partners.[19]
The high intelligence, initiative and self-direction of Labradors in working roles is exemplified by dogs such as Endal, who was trained to, if need be, put his human who uses a wheelchair in the recovery position, cover him with a blanket, and activate an emergency phone.[20] A number of Labradors have also been taught to assist their owner in removing money and credit cards from ATMs with prior training.[21]
The breed is used in water rescue/lifesaving. It continues in that role today, along with the Leonberger, Newfoundland and Golden Retriever dogs; they are used at the Italian School of Canine Lifeguard.
A Labrador may be expected to live for 10 to 12 years.[1]: 256
It is a healthy breed with relatively few major problems. Notable issues related to health and well-being include inherited disorders and obesity (most are missing all or parts of the appetite-regulating POMC gene).


SEGMENT
A Royal Veterinary College study, and one conducted by The University of Sydney, have concluded that chocolate-coloured Labradors have a shorter average life expectancy than other colours of Labrador (by about 10%) and are more likely to suffer some health problems.[24]
Labradors are somewhat prone to hip and elbow dysplasia,[25] especially in larger dogs.[26] Eye diseases may include progressive retinal atrophy, cataracts, corneal dystrophy[25] and retinal dysplasia.[27] They can suffer from exercise induced collapse, which causes hyperthermia, weakness, collapse, and disorientation after short bouts of exercise,[28] or from obesity, which in some cases may be partly due to the absence of part or all of the proopiomelanocortin gene.
Indie rock is a subgenre of rock music that originated in the United States, United Kingdom and New Zealand from the 1970s to the 1980s.[1] Originally used to describe independent record labels, the term became associated with the music they produced and was initially used interchangeably with alternative rock or "guitar pop rock".[2]
One of the primary scenes of the movement was Dunedin,[3] where a cultural scene based around a convergence of noise pop and jangle became popular among the city's large student population.[4][5][6] Independent labels such as Flying Nun began to promote the scene across New Zealand, inspiring key college rock bands in the United States such as Pavement, Pixies and R.E.M.[7][8] Other notable scenes grew in Manchester and Hamburg, with many others thriving thereafter.
In the 1980s, the use of the term "indie" (or "indie pop") started to shift from its reference to recording companies to describe the style of music produced on punk and post-punk labels.[9] During the 1990s, grunge and punk revival bands in the US and Britpop bands in the UK broke into the mainstream, and the term "alternative" lost its original counter-cultural meaning. The term "indie rock" became associated with the bands and genres that remained dedicated to their independent status.[10] By the end of the 1990s, indie rock developed several subgenres and related styles, including lo-fi, noise pop, emo, slowcore, post-rock, and math rock.[10] In the 2000s, changes in the music industry and the growing importance of the internet enabled a new wave of indie rock bands to achieve mainstream success, leading to questions about its meaningfulness as a term.[11]
In the early 2000s, a new group of bands that played a stripped-down, back-to-basics version of guitar rock emerged into the mainstream. The commercial breakthrough from these scenes was led by four bands: The Strokes, The White Stripes, The Hives and The Vines. Emo also broke into mainstream culture in the early 2000s.[12] By the end of the decade, the proliferation of indie bands was being referred to as an "indie landfill",[13] with the term "landfill indie" becoming used by some critics/websites in the 2020s as subgenre for a certain type of 2000s indie band, in the same way Britpop is used for British guitar music of the 1990s.
The term indie rock, which comes from "independent", describes the small and relatively low-budget labels on which it is released and the do-it-yourself attitude of the bands and artists involved. Although distribution deals are often struck with major corporate companies, these labels and the bands they host have attempted to retain their autonomy, leaving them free to explore sounds, emotions and subjects of limited appeal to large, mainstream audiences.[10] The influences and styles of the artists have been extremely diverse, including punk, psychedelia, post-punk and country.[9] The terms "alternative rock" and "indie rock" were used interchangeably in the 1980s, but after many alternative bands followed Nirvana into the mainstream in the early 1990s, "indie rock" began to be used to describe those bands, working in a variety of styles, that did not pursue or achieve commercial success.[10] Aesthetically speaking, indie rock is characterized as having a careful balance of pop accessibility with noise, experimentation with pop music formulae, sensitive lyrics masked by ironic posturing, a concern with authenticity, and the depiction of a simple guy or girl.[17]
Allmusic identifies indie rock as including a number of "varying musical approaches [not] compatible with mainstream tastes".[18] Linked by an ethos more than a musical approach, the indie rock movement encompassed a wide range of styles, from hard-edged, grunge-influenced bands, through do-it-yourself experimental bands like Pavement, to punk-folk singers such as Ani DiFranco.[19] In fact, there is an everlasting list of genres and subgenres of indie rock.[20] Many countries have developed an extensive local indie scene, flourishing with bands with enough popularity to survive inside the respective country, but virtually unknown elsewhere. However, there are still indie bands that start off locally, but eventually attract an international audience.[21][22]
Indie rock is noted for having a relatively high proportion of female artists compared with preceding rock genres, a tendency exemplified by the development of the feminist-informed riot grrrl music of acts like Bikini Kill, Bratmobile, 7 Year Bitch, Team Dresch and Huggy Bear.[23] However, Cortney Harding pointed out that this sense of equality is not reflected in the number of women running indie labels.
Post-punk and indie pop
The BBC documentary Music for Misfits: The Story of Indie[25] pinpoints the birth of indie as the 1977 self-publication of the Spiral Scratch EP by Manchester band Buzzcocks. Although Buzzcocks are often classified as a punk band, it has been argued by the BBC and others[26] that the publication of Spiral Scratch independently of a major label led to the coining of the name "indie" ("indie" being the shortened form of "independent").
"Indie pop" and "indie" were originally synonymous.[27] In the mid-1980s, "indie" began to be used to describe the music produced on post-punk labels rather than the labels themselves.[9] The indie rock scene in the US was prefigured by the college rock[28] that dominated college radio playlists, which included key bands like R.E.M. from the US and The Smiths from the UK.[29] These two bands rejected the dominant synthpop of the early 1980s,[30][31] and helped inspire guitar-based jangle pop; other important bands in the genre included 10,000 Maniacs and the dB's from the US, and The Housemartins and The La's from the UK. In the United States, the term was particularly associated with the abrasive, distortion-heavy sounds of the Pixies, Hüsker Dü, Minutemen, Meat Puppets, Dinosaur Jr., and The Replacements.[29]
In the United Kingdom the C86 cassette, a 1986 NME compilation featuring Primal Scream, The Pastels, The Wedding Present and other bands, was a document of the UK indie scene. It gave its name to the indie pop scene that followed, which was a major influence on the development of the British indie scene as a whole.[32][33] Major precursors of indie pop included Postcard bands Josef K and Orange Juice, and significant labels included Creation, Subway and Glass.[27] The Jesus and Mary Chain's sound combined the Velvet Underground's "melancholy noise" with Beach Boys pop melodies and Phil Spector's "Wall of Sound" production,[34][35][importance of example(s)?] while New Order emerged from the demise of post-punk band Joy Division and experimented with techno and house music.
Noise rock and shoegazing
The most abrasive and discordant outgrowth of punk was noise rock, which emphasised loud distorted electric guitars and powerful drums, and was pioneered by bands including Sonic Youth, Big Black and Butthole Surfers.[37]
Swans, an influential band from New York, is identified as part of the No Wave scene which included Lydia Lunch, and James Chance & The Contortions. These bands were documented by Brian Eno on the seminal compilation album No New York. A number of prominent indie rock record labels were founded during the 1980s. These include Washington, D.C.'s Dischord Records in 1980, Seattle's Sub Pop Records in 1986[38] and New York City's Matador Records and Durham, North Carolina's Merge Records in 1989. Chicago's Touch and Go Records was founded as a fanzine in 1979 and began to release records during the 1980s.[39]
The Jesus and Mary Chain, along with Dinosaur Jr, indie pop and the dream pop of Cocteau Twins, were the formative influences for the shoegazing movement of the late 1980s. Named for the band members' tendency to stare at their feet and guitar effects pedals onstage rather than interact with the audience, acts like My Bloody Valentine, and later Slowdive and Ride created a loud "wash of sound" that obscured vocals and melodies with long, droning riffs, distortion, and feedback.[40]
The other major movement at the end of the 1980s was the drug-fuelled Madchester scene. Based around The Haçienda, a nightclub in Manchester owned by New Order and Factory Records, Madchester bands such as Happy Mondays and The Stone Roses mixed acid house dance rhythms, Northern soul and funk with melodic guitar pop.
Alternative enters the mainstream
The 1990s brought major changes to the alternative rock scene. Grunge bands such as Nirvana, Pearl Jam, Soundgarden, and Alice in Chains broke into the mainstream, achieving commercial chart success and widespread exposure.[10] Punk revival bands like Green Day and The Offspring also became popular and were grouped under the "alternative" umbrella.[19] Similarly, in the United Kingdom Britpop saw bands like Blur and Oasis emerge into the mainstream, abandoning the regional, small-scale and political elements of the 1980s indie scene.[42] Bands like Hüsker Dü and Violent Femmes were just as prominent during this time period, yet they have remained iconoclastic, and are not the bands that are frequently cited as inspirations to the current generation of indie rockers.[43]
As a result of alternative rock bands moving into the mainstream, the term "alternative" lost its original counter-cultural meaning and began to refer to the new, commercially lighter form of music that was now achieving mainstream success. It has been argued that even the term "sellout" lost its meaning as grunge made it possible for a niche movement, no matter how radical, to be co-opted by the mainstream, cementing the formation of an individualist, fragmented culture.[44] It is argued that staying independent became a career choice for bands privy to industry functions rather than an ideal, as resistance to the market evaporated in favor of a more synergistic culture.[44]
Indie electronic
Indie electronic or indietronica[57] covers rock-based artists who share an affinity for electronic music, using samplers, synthesizers, drum machines, and computer programs.[56] Less a style and more broad a categorization, it describes an early 1990s trend of acts who followed in the traditions of early electronic music (composers of the BBC Radiophonic Workshop), krautrock and synth-pop.[56] Progenitors of the genre were English bands Disco Inferno, Stereolab, and Space.[56] Most musicians in the genre can be found on independent labels like Warp, Morr Music, Sub Pop or Ghostly International.[56] Examples include Broadcast, MGMT, LCD Soundsystem and Animal Collective.
Diversification
By the end of the 1990s, indie rock developed a number of subgenres and related styles. Following indie pop, these included lo-fi, noise pop, sadcore, post-rock, space rock and math rock.[10] The work of Talk Talk and Slint helped inspire post-rock (an experimental style influenced by jazz and electronic music, pioneered by Bark Psychosis and taken up by acts such as Tortoise, Stereolab, and Laika),[58][59] as well as leading to more dense and complex, guitar-based math rock, developed by acts like Polvo and Chavez.[60] Built to Spill's 1999 album Keep It Like a Secret helped to shape the indie-rock sound of the early 2000s.[61]
Space rock looked back to progressive roots, with drone-heavy and minimalist acts like Spacemen 3 in the 1980s, Spectrum and Spiritualized, and later groups including Flying Saucer Attack, Godspeed You! Black Emperor and Quickspace.[62] In contrast, sadcore emphasized pain and suffering through melodic use of acoustic and electronic instrumentation in the music of bands like American Music Club and Red House Painters.[63]
The revival of Baroque pop reacted against lo-fi and experimental music by placing an emphasis on melody and classical instrumentation, with artists like Arcade Fire, Belle and Sebastian, Rufus Wainwright, Beirut and The Decemberists.
Weezer's Pinkerton (1996) introduced the Emo genre to a wider and more mainstream audience.
Post-punk revival
In the early 2000s, a new group of bands that played a stripped-down and back-to-basics version of guitar rock emerged into the mainstream, which some termed a post-punk revival, but because the bands came from across the globe, cited diverse influences (from traditional blues, through new wave to grunge), and adopted differing styles of dress, their unity as a genre has been disputed.[79]
The commercial breakthrough of the genre came in early 2000s with the success of; The Strokes, The White Stripes, The Vines, and The Hives. They were christened by parts of the media as the "The" bands, and dubbed "the saviours of rock 'n' roll",[80] prompting Rolling Stone magazine to declare on its September 2002 cover, "Rock is Back!"[81]
A second wave of bands that managed to gain international recognition as a result of the movement included Interpol, the Black Keys, the Killers, Kings of Leon, Modest Mouse, the Shins, the Bravery, Spoon, the Hold Steady, and the National in the US,[82] and Franz Ferdinand, Bloc Party, the Futureheads, The Cribs, the Libertines,[83] Kaiser Chiefs and the Kooks in the UK.[84] Arctic Monkeys were the most prominent act to owe their initial commercial success to the use of Internet social networking,[85] with two No. 1 singles and Whatever People Say I Am, That's What I'm Not (2006), which became the fastest-selling debut album in British chart history.
