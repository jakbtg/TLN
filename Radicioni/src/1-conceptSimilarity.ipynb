{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Concept Similarity with WordNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wu & Palmer Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth of a synset - number of steps from the synset to the root\n",
    "def depth(s):\n",
    "    if s is None:\n",
    "        return -1\n",
    "    hypernyms = s.hypernyms()\n",
    "    if len(hypernyms) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 + max([depth(h) for h in hypernyms]) # add 1 to get the last step to the synset s\n",
    "\n",
    "# Get all hypernyms of a synset (including itself)\n",
    "def all_hypernyms(s):\n",
    "    hypernyms = [s]\n",
    "    for h in s.hypernyms():\n",
    "        hypernyms.extend(all_hypernyms(h))\n",
    "    return hypernyms\n",
    "\n",
    "# Lowest Common Subsumer\n",
    "def lcs(s1, s2):\n",
    "    hyper1 = all_hypernyms(s1)\n",
    "    hyper2 = all_hypernyms(s2)\n",
    "    if len(hyper1) == 0 or len(hyper2) == 0:\n",
    "        return None\n",
    "    candidates = set(hyper1).intersection(set(hyper2))\n",
    "    if len(candidates) > 0:\n",
    "        return max(candidates, key=depth)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Wu and Palmer similarity\n",
    "def wup_sim(s1, s2):\n",
    "    lcs_syn = lcs(s1, s2)\n",
    "    if lcs_syn is None:\n",
    "        return 0\n",
    "    return 2 * depth(lcs_syn) / (depth(s1) + depth(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest Path Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max depth of all synsets in wordnet\n",
    "def max_depth():\n",
    "    max = 0\n",
    "    for synset in wn.all_synsets():\n",
    "        d = depth(synset)\n",
    "        if d > max:\n",
    "            max = d\n",
    "    return max\n",
    "\n",
    "# max_depth = max_depth() it is always 19, so we can hardcode it\n",
    "max_depth = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find shortest path between two synsets\n",
    "def shortest_path(s1, s2):\n",
    "    lcs_syn = lcs(s1, s2)\n",
    "    if lcs_syn is None:\n",
    "        return max_depth\n",
    "    return depth(s1) + depth(s2) - 2 * depth(lcs_syn)\n",
    "\n",
    "# Shortest path similarity\n",
    "def short_path_sim(s1, s2):\n",
    "    return 2 * max_depth - shortest_path(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leacock & Chodorow Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leacock and Chodorow similarity\n",
    "def leac_sim(s1, s2):\n",
    "    distance = shortest_path(s1, s2)\n",
    "    if distance == 0:\n",
    "        return -np.log(distance + 1 / (2 * max_depth + 1))\n",
    "    return -np.log(distance / (2 * max_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato che l'input in WordSim353 Ã¨ una coppia di termini, mentre le tre formule utilizzano sensi, per calcolare la similarity fra i 2 termini prendiamo la massima similarity fra tutti i sensi del primo termine e tutti i sensi del secondo termine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max similarity between all synsets of two words, given a similarity function\n",
    "def get_max_similarity(s1, s2, sim_func):\n",
    "    s1_synsets = wn.synsets(s1)\n",
    "    s2_synsets = wn.synsets(s2)\n",
    "    max_sim = 0\n",
    "    # best_s1 = None\n",
    "    # best_s2 = None\n",
    "    for s1_syn in s1_synsets:\n",
    "        for s2_syn in s2_synsets:\n",
    "            sim = sim_func(s1_syn, s2_syn)\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                # best_s1 = s1_syn\n",
    "                # best_s2 = s2_syn\n",
    "    return max_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('/Users/jak/Documents/Uni/TLN/TLN /Radicioni/data/WordSim353.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarities for WordSim353 and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Word 1         Word 2  Human (mean)       wup  shortest   leacock\n",
      "0        love            sex          6.77  0.909091        37  3.637586\n",
      "1       tiger            cat          7.35  0.962963        37  3.637586\n",
      "2       tiger          tiger         10.00  1.000000        38  3.663562\n",
      "3        book          paper          7.46  0.857143        36  2.944439\n",
      "4    computer       keyboard          7.62  0.800000        35  2.538974\n",
      "5    computer       internet          7.58  0.588235        31  1.691676\n",
      "6       plane            car          5.77  0.700000        32  1.845827\n",
      "7       train            car          6.31  0.705882        33  2.028148\n",
      "8   telephone  communication          7.50  0.000000        28  1.335001\n",
      "9  television          radio          6.77  0.900000        36  2.944439\n",
      "\n",
      "Wu and Palmer similarity results\n",
      "Spearman correlation: 0.32205979107230365, p-value: 5.816296751820106e-10\n",
      "Pearson correlation: 0.2756318557372003, p-value: 1.4205427728723012e-07\n",
      "--------------------------------\n",
      "Shortest Path similarity results\n",
      "Spearman correlation: 0.2725348694492164, p-value: 1.9812858428748427e-07\n",
      "Pearson correlation: 0.15778620904840207, p-value: 0.002952296149109044\n",
      "--------------------------------\n",
      "Leacock and Chodorow similarity results\n",
      "Spearman correlation: 0.2725348694492164, p-value: 1.9812858428748427e-07\n",
      "Pearson correlation: 0.3052335447838761, p-value: 4.776364600146794e-09\n"
     ]
    }
   ],
   "source": [
    "# Compute the three similarity measures\n",
    "df['wup'] = df.apply(lambda row: get_max_similarity(row['Word 1'], row['Word 2'], wup_sim), axis=1)\n",
    "df['shortest'] = df.apply(lambda row: get_max_similarity(row['Word 1'], row['Word 2'], short_path_sim), axis=1)\n",
    "df['leacock'] = df.apply(lambda row: get_max_similarity(row['Word 1'], row['Word 2'], leac_sim), axis=1)\n",
    "\n",
    "# Show the dataframe\n",
    "print(df.head(10))\n",
    "\n",
    "# Results for Wu and Palmer similarity\n",
    "print('\\nWu and Palmer similarity results')\n",
    "spearman = stats.spearmanr(df['Human (mean)'], df['wup'])\n",
    "print(f'Spearman correlation: {spearman[0]}, p-value: {spearman[1]}')\n",
    "pearson = stats.pearsonr(df['Human (mean)'], df['wup'])\n",
    "print(f'Pearson correlation: {pearson[0]}, p-value: {pearson[1]}')\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "# Results for Shortest Path similarity\n",
    "print('Shortest Path similarity results')\n",
    "spearman = stats.spearmanr(df['Human (mean)'], df['shortest'])\n",
    "print(f'Spearman correlation: {spearman[0]}, p-value: {spearman[1]}')\n",
    "pearson = stats.pearsonr(df['Human (mean)'], df['shortest'])\n",
    "print(f'Pearson correlation: {pearson[0]}, p-value: {pearson[1]}')\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "# Results for Leacock and Chodorow similarity\n",
    "print('Leacock and Chodorow similarity results')\n",
    "spearman = stats.spearmanr(df['Human (mean)'], df['leacock'])\n",
    "print(f'Spearman correlation: {spearman[0]}, p-value: {spearman[1]}')\n",
    "pearson = stats.pearsonr(df['Human (mean)'], df['leacock'])\n",
    "print(f'Pearson correlation: {pearson[0]}, p-value: {pearson[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5587bf738ccec4bb524b17f914a1c07eb93a384af8fabdf9504251bc61fb6171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
