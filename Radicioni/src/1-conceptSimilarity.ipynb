{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Concept Similarity with WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementazione di 3 misure di similarità basate su WordNet:  \n",
    "* Wu-Palmer\n",
    "* Shortest Path\n",
    "* Leacock-Chodorow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[Synset('prognosis.n.01'), Synset('prediction.n.02'), Synset('statement.n.01'), Synset('message.n.02'), Synset('communication.n.02'), Synset('abstraction.n.06'), Synset('entity.n.01')]\n",
      "[Synset('weather.n.01'), Synset('atmospheric_phenomenon.n.01'), Synset('physical_phenomenon.n.01'), Synset('natural_phenomenon.n.01'), Synset('phenomenon.n.01'), Synset('process.n.06'), Synset('physical_entity.n.01'), Synset('entity.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# Depth of a synset\n",
    "def depth(s):\n",
    "    if s is None:\n",
    "        return -1\n",
    "    hypernyms = s.hypernyms()\n",
    "    if len(hypernyms) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 + max([depth(h) for h in hypernyms]) # add 1 to get the last step to the synset s\n",
    "\n",
    "# Get all hypernyms of a synset (including itself)\n",
    "def get_hypernyms(s):\n",
    "    hypernyms = [s]\n",
    "    for h in s.hypernyms():\n",
    "        hypernyms.extend(get_hypernyms(h))\n",
    "    return hypernyms\n",
    "\n",
    "# Lowest Common Subsumer\n",
    "def lcs(s1, s2):\n",
    "    hyper1 = get_hypernyms(s1)\n",
    "    hyper2 = get_hypernyms(s2)\n",
    "    if len(hyper1) == 0 or len(hyper2) == 0:\n",
    "        return None\n",
    "    candidates = set(hyper1).intersection(set(hyper2))\n",
    "    if len(candidates) > 0:\n",
    "        return max(candidates, key=depth)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Wu and Palmer similarity\n",
    "def wup_sim(s1, s2):\n",
    "    lcs_syn = lcs(s1, s2)\n",
    "    if lcs_syn is None:\n",
    "        return 0\n",
    "    return 2 * depth(lcs_syn) / (depth(s1) + depth(s2))\n",
    "\n",
    "# Get max similarity between all synsets of two words\n",
    "def get_max_similarity(s1, s2):\n",
    "    s1_synsets = wn.synsets(s1)\n",
    "    s2_synsets = wn.synsets(s2)\n",
    "    max_sim = 0\n",
    "    # best_s1 = None\n",
    "    # best_s2 = None\n",
    "    for s1_synset in s1_synsets:\n",
    "        for s2_synset in s2_synsets:\n",
    "            sim = wup_sim(s1_synset, s2_synset)\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                # best_s1 = s1_synset\n",
    "                # best_s2 = s2_synset\n",
    "    return max_sim\n",
    "\n",
    "\n",
    "# get lcs for dog and cat\n",
    "# print(get_lcs(wn.synset('cat.n.01'), wn.synset('cat.n.01')))\n",
    "# print(get_max_similarity('dog', 'cat'))\n",
    "print(depth(wn.synset('forecast.n.01')))\n",
    "print(get_hypernyms(wn.synset('forecast.n.01')))\n",
    "print(get_hypernyms(wn.synset('weather.n.01')))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato che l'input in WordSim353 è una coppia di termini, mentre le tre formule utilizzano sensi, per calcolare la similarity fra i 2 termini prendiamo la massima similarity fra tutti i sensi del primo termine e tutti i sensi del secondo termine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('/Users/jak/Documents/Uni/TLN/TLN /Radicioni/data/WordSim353.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Word 1    Word 2  Human (mean)  max_similarity  max_similarity2\n",
      "0            love       sex          6.77        0.909091         0.923077\n",
      "1           tiger       cat          7.35        0.962963         0.965517\n",
      "2           tiger     tiger         10.00        1.000000         1.000000\n",
      "3            book     paper          7.46        0.857143         0.875000\n",
      "4        computer  keyboard          7.62        0.800000         0.823529\n",
      "..            ...       ...           ...             ...              ...\n",
      "348        shower     flood          6.03        0.600000         0.636364\n",
      "349       weather  forecast          8.34        0.000000         0.333333\n",
      "350      disaster      area          6.25        0.428571         0.500000\n",
      "351      governor    office          6.34        0.470588         0.526316\n",
      "352  architecture   century          3.78        0.181818         0.307692\n",
      "\n",
      "[353 rows x 5 columns]\n",
      "SpearmanrResult(correlation=0.32205979107230365, pvalue=5.816296751820106e-10)\n",
      "SpearmanrResult(correlation=0.33850273836806116, pvalue=6.527256994582947e-11)\n",
      "(0.2756318557372003, 1.4205427728723012e-07)\n",
      "(0.297440832551365, 1.2117891412856982e-08)\n"
     ]
    }
   ],
   "source": [
    "# Prova la get_max_similarity su tutte le coppie di parole\n",
    "df['max_similarity'] = df.apply(lambda row: get_max_similarity(row['Word 1'], row['Word 2']), axis=1)\n",
    "\n",
    "# get max similarity between all synsets of two words using NLTK wup_similarity\n",
    "def get_max_sim2(s1, s2):\n",
    "    s1_synsets = wn.synsets(s1)\n",
    "    s2_synsets = wn.synsets(s2)\n",
    "    max_sim = 0\n",
    "    for s1_synset in s1_synsets:\n",
    "        for s2_synset in s2_synsets:\n",
    "            sim = s1_synset.wup_similarity(s2_synset)\n",
    "            if sim is not None and sim > max_sim:\n",
    "                max_sim = sim\n",
    "    return max_sim\n",
    "\n",
    "# Prova la get_max_sim2 su tutte le coppie di parole\n",
    "df['max_similarity2'] = df.apply(lambda row: get_max_sim2(row['Word 1'], row['Word 2']), axis=1)\n",
    "\n",
    "# Show the results\n",
    "print(df)\n",
    "\n",
    "# use scipy.stats.spearmanr to calculate the Spearman correlation\n",
    "print(stats.spearmanr(df['Human (mean)'], df['max_similarity']))\n",
    "print(stats.spearmanr(df['Human (mean)'], df['max_similarity2']))\n",
    "\n",
    "# use scipy.stats.pearsonr to calculate the Pearson correlation\n",
    "print(stats.pearsonr(df['Human (mean)'], df['max_similarity']))\n",
    "print(stats.pearsonr(df['Human (mean)'], df['max_similarity2']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max similarity for each word pair from all synsets\n",
    "def get_max_similarity(word1, word2):\n",
    "    max_similarity = 0\n",
    "    for synset1 in wn.synsets(word1):\n",
    "        for synset2 in wn.synsets(word2):\n",
    "            similarity = synset1.path_similarity(synset2)\n",
    "            if similarity is not None and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "    return max_similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5587bf738ccec4bb524b17f914a1c07eb93a384af8fabdf9504251bc61fb6171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
