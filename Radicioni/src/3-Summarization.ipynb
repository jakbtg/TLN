{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Automatic Summarization**\n",
    "\n",
    "Algortimo semplice:\n",
    "1. individua argomento del testo come una lista di vettori di Nasari (termine1, score1, termine2, score2, ...)\n",
    "2. crea contesto raccogliendo vettori dei termini trovati al passo prima. Nel caso in cui il titolo sia troppo corto (e quindi poco informativo) posso cercare vettori di elementi che fanno parte delle definizioni dei termini trovati al passo 1\n",
    "3. peso i paragrafi in base alla somma dei pesi dei termini che fanno parte del contesto con la Weighted Overlap. Mantengo solo i paragrafi che hanno un peso maggiore di una soglia\n",
    "\n",
    "Valutazione:\n",
    "- BLEU\n",
    "- ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni di utility per il preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['system', 'test', 'pre-processing']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Useful to remove punctuation from first or last char of a token \n",
    "# - Esempio: senza questa funzione \"It's\" diventa \"It\" e \"'s\"\n",
    "# - It viene eliminato perchè è una stopword, mentre 's non viene eliminato perchè non rientra nè tra le stopwords nè tra la punteggiatura\n",
    "# - Con questa funzione rimuovo ' da 's e poi rimuovo nuovamente eventuali stopwords.\n",
    "def remove_first_last(tokens, punct, stop):\n",
    "    for i in range(len(tokens)):\n",
    "        for p in punct:\n",
    "            if tokens[i].startswith(p):\n",
    "                tokens[i] = tokens[i][1:]\n",
    "            if tokens[i].endswith(p):\n",
    "                tokens[i] = tokens[i][:-1]\n",
    "    tokens = [t for t in tokens if t not in stop]\n",
    "    return tokens\n",
    "\n",
    "# Remove stopwords and punctuation from the text, tokenize it and lemmatize it\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    stop = []\n",
    "    with open('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/stop_words_FULL.txt', 'r') as f:\n",
    "        stop = f.read().splitlines()\n",
    "    stop = set(stop)\n",
    "    punct = ['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '``', \"''\", '...', '’', '“', '”']\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop and t not in punct]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = list(set([lemmatizer.lemmatize(t) for t in tokens]))\n",
    "    tokens = remove_first_last(tokens, punct, stop)\n",
    "    return tokens\n",
    "\n",
    "# try preprocessing\n",
    "preprocess(\"This is a test. It's a test of the pre-processing system.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5587bf738ccec4bb524b17f914a1c07eb93a384af8fabdf9504251bc61fb6171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
