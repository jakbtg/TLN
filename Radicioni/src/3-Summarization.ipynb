{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Automatic Summarization**\n",
    "\n",
    "Algortimo semplice:\n",
    "1. individua argomento del testo come una lista di vettori di Nasari (termine1, score1, termine2, score2, ...)\n",
    "2. crea contesto raccogliendo vettori dei termini trovati al passo prima. Nel caso in cui il titolo sia troppo corto (e quindi poco informativo) posso cercare vettori di elementi che fanno parte delle definizioni dei termini trovati al passo 1\n",
    "3. peso i paragrafi in base alla somma dei pesi dei termini che fanno parte del contesto con la Weighted Overlap. Mantengo solo i paragrafi che hanno un peso maggiore di una soglia\n",
    "\n",
    "Valutazione:\n",
    "- BLEU\n",
    "- ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni di utility per il preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'system', 'pre-processing']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Useful to remove punctuation from first or last char of a token \n",
    "# - Esempio: senza questa funzione \"It's\" diventa \"It\" e \"'s\"\n",
    "# - It viene eliminato perchè è una stopword, mentre 's non viene eliminato perchè non rientra nè tra le stopwords nè tra la punteggiatura\n",
    "# - Con questa funzione rimuovo ' da 's e poi rimuovo nuovamente eventuali stopwords.\n",
    "def remove_first_last(tokens, punct, stop):\n",
    "    for i in range(len(tokens)):\n",
    "        for p in punct:\n",
    "            if tokens[i].startswith(p):\n",
    "                tokens[i] = tokens[i][1:]\n",
    "            if tokens[i].endswith(p):\n",
    "                tokens[i] = tokens[i][:-1]\n",
    "    tokens = [t for t in tokens if t not in stop]\n",
    "    return tokens\n",
    "\n",
    "# Remove stopwords and punctuation from the text, tokenize it and lemmatize it\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    stop = []\n",
    "    with open('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/stop_words_FULL.txt', 'r') as f:\n",
    "        stop = f.read().splitlines()\n",
    "    stop = set(stop)\n",
    "    punct = ['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '``', \"''\", '...', '’', '“', '”', '‘']\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop and t not in punct]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = list(set([lemmatizer.lemmatize(t) for t in tokens]))\n",
    "    tokens = remove_first_last(tokens, punct, stop)\n",
    "    return tokens\n",
    "\n",
    "# try preprocessing\n",
    "preprocess(\"This is a test. It's a test of the pre-processing system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing del file di input Nasari e creazione dizionario di vettori Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smoking': '392.73',\n",
       " 'tobacco': '264.75',\n",
       " 'smoke-free': '212.34',\n",
       " 'smoke': '190.01',\n",
       " 'tobacco industry': '137.74',\n",
       " 'ban': '105.97',\n",
       " 'second-hand': '97.45',\n",
       " 'packaging': '70.9',\n",
       " 'cigarette': '69.96',\n",
       " 'smoker': '67.8',\n",
       " 'foxley-norris': '61.4',\n",
       " 'exposure': '60.91',\n",
       " 'plain': '45.4',\n",
       " 'forest': '44.65'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsing the Nasari file and creating a dictionary with:\n",
    "# - key: word\n",
    "# - value: dictionary with:\n",
    "#          - key: lemma\n",
    "#          - value: score\n",
    "nasari = {}\n",
    "with open('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/dd-small-nasari-15.txt', 'r') as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "    for line in lines:\n",
    "        line = line.split(';')\n",
    "        tmp = {}\n",
    "        for lemma in line[2:]:\n",
    "            lemma = lemma.split('_')\n",
    "            if len(lemma) > 1:\n",
    "                tmp[lemma[0]] = lemma[1]\n",
    "        nasari[line[1].lower()] = tmp\n",
    "\n",
    "# nasari['forest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvo i documenti di input\n",
    "\n",
    "Rendo ogni documento una lista di paragrafi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save document\n",
    "def save_doc(filename):\n",
    "    doc = []\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = [line.rstrip('\\n') for line in f]\n",
    "        for line in lines:\n",
    "            if '#' not in line and line != '': # remove empty lines and the first line with the link\n",
    "                doc.append(line)\n",
    "    return doc\n",
    "\n",
    "# try save_doc\n",
    "# save_doc('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Individuate the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people', 'coping', 'indoors', 'life']\n"
     ]
    }
   ],
   "source": [
    "# Get title from document, considering the first line\n",
    "def get_title(filename):\n",
    "    doc = save_doc(filename)\n",
    "    return preprocess(doc[0])\n",
    "\n",
    "# Get topic words from the text checking if they are in the Nasari dictionary\n",
    "def get_topic_words(text):\n",
    "    tokens = preprocess(text)\n",
    "    topic_words = [t for t in tokens if t in nasari.keys()]\n",
    "    return topic_words\n",
    "\n",
    "# Get random paragraph topic words from the document (not the title)\n",
    "def get_random_paragraph(filename):\n",
    "    doc = save_doc(filename)\n",
    "    paragraph = random.choice(doc[1:])\n",
    "    topic_words = get_topic_words(paragraph)\n",
    "    return topic_words\n",
    "\n",
    "print(get_title('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Life-indoors.txt'))\n",
    "# get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the context for a document title \n",
    "# - It return a list of dictionaries associated to the topic words of the title if they are in the Nasari dictionary\n",
    "def create_title_context(filename):\n",
    "    title = get_title(filename)\n",
    "    topic_words = get_topic_words(title)\n",
    "    context_vector = [nasari[word] for word in topic_words]\n",
    "    return context_vector\n",
    "\n",
    "# try create_context\n",
    "# print(create_title_context('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt'))\n",
    "\n",
    "# PROBABLY NOT NEEDED\n",
    "# Create the context for a paragraph\n",
    "# - It returns a list of dictionaries associated to the topic words of the paragraph if they are in the Nasari dictionary\n",
    "def create_paragraph_context(paragraph):\n",
    "    topic_words = get_topic_words(paragraph)\n",
    "    context_vector = [nasari[word] for word in topic_words]\n",
    "    return context_vector\n",
    "\n",
    "# try create_paragraph_context\n",
    "# for i in range(10):\n",
    "#     topic_words, paragraph = get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Life-indoors.txt')\n",
    "#     print(topic_words)\n",
    "#     # print(paragraph)\n",
    "#     print(create_paragraph_context(paragraph))\n",
    "# create_paragraph_context(get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retain paragraphs whose sentences contain the most salient terms, based on the Weighted Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['people', 'coping', 'indoors', 'life']\n",
      "Paragraph: (['musician', 'performance', 'week'], 'Musicians from the Beijing-based Chinese group The 2econd could not meet for weeks, but have now been able to come together and livestream a performance for their fans.')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vy/61q2hg6j75vdf08b96dmw_6c0000gn/T/ipykernel_76239/584616117.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# try get_overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrand_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Life-indoors.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_overlap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Life-indoors.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_top\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Get rank as the position of a lemma in the vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vy/61q2hg6j75vdf08b96dmw_6c0000gn/T/ipykernel_76239/584616117.py\u001b[0m in \u001b[0;36mget_overlap\u001b[0;34m(text, paragraph)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Text: {text}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Paragraph: {paragraph}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mparagraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Text: {text}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vy/61q2hg6j75vdf08b96dmw_6c0000gn/T/ipykernel_76239/3593248977.py\u001b[0m in \u001b[0;36mget_topic_words\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Get topic words from the text checking if they are in the Nasari dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtopic_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnasari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtopic_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vy/61q2hg6j75vdf08b96dmw_6c0000gn/T/ipykernel_76239/953469457.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Remove stopwords and punctuation from the text, tokenize it and lemmatize it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/stop_words_FULL.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Get overlap between a text topic words and a paragraph topic words\n",
    "def get_overlap(text, paragraph):\n",
    "    print(f'Text: {text}')\n",
    "    print(f'Paragraph: {paragraph}')\n",
    "    text = get_topic_words(text)\n",
    "    paragraph = get_topic_words(paragraph)\n",
    "    print(f'Text: {text}')\n",
    "    print(f'Paragraph: {paragraph}')\n",
    "    return text and paragraph\n",
    "\n",
    "# try get_overlap\n",
    "rand_top = get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Life-indoors.txt')\n",
    "print(get_overlap(get_title('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Life-indoors.txt'), rand_top))\n",
    "\n",
    "# Get rank as the position of a lemma in the vector\n",
    "def get_rank(lemma, vector):\n",
    "    for i in range(len(vector)):\n",
    "        if lemma == vector[i]:\n",
    "            return i + 1\n",
    "    print('ever here?')\n",
    "    return None\n",
    "# def get_rank(term, vector):\n",
    "#     for i in range(len(vector)):\n",
    "#         print(term)\n",
    "#         print(vector[i])\n",
    "#         if term in vector[i].keys():\n",
    "#             return i + 1\n",
    "#     return None # If the term is not in the vector\n",
    "\n",
    "# try get_rank\n",
    "# print(get_rank('virus', create_context('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')))\n",
    "\n",
    "# Compute weighted overlap between two vectors\n",
    "def weighted_overlap(v1, v2):\n",
    "    overlap = get_overlap(v1, v2)\n",
    "    print(f'overlap: {overlap}')\n",
    "    if overlap:\n",
    "        i = 0\n",
    "        num = 0\n",
    "        den = 0\n",
    "        for term in overlap:\n",
    "            den += get_rank(term, v1) + get_rank(term, v2) # This should be the num but since it is to the power of -1 I can put it in the den\n",
    "            num += 2 * i # This should be the den but since it is to the power of -1 I can put it in the num\n",
    "            i += 1\n",
    "        return num / den\n",
    "    return 0\n",
    "\n",
    "# try weighted_overlap\n",
    "# weighted_overlap(get_title('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt'), get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5587bf738ccec4bb524b17f914a1c07eb93a384af8fabdf9504251bc61fb6171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
