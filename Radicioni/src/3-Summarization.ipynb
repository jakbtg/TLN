{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Automatic Summarization**\n",
    "\n",
    "Algortimo semplice:\n",
    "1. individua argomento del testo come una lista di vettori di Nasari (termine1, score1, termine2, score2, ...)\n",
    "2. crea contesto raccogliendo vettori dei termini trovati al passo prima. Nel caso in cui il titolo sia troppo corto (e quindi poco informativo) posso cercare vettori di elementi che fanno parte delle definizioni dei termini trovati al passo 1\n",
    "3. peso i paragrafi in base alla somma dei pesi dei termini che fanno parte del contesto con la Weighted Overlap. Mantengo solo i paragrafi che hanno un peso maggiore di una soglia\n",
    "\n",
    "Valutazione:\n",
    "- BLEU\n",
    "- ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni di utility per il preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['system', 'test', 'pre-processing']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Useful to remove punctuation from first or last char of a token \n",
    "# - Esempio: senza questa funzione \"It's\" diventa \"It\" e \"'s\"\n",
    "# - It viene eliminato perchè è una stopword, mentre 's non viene eliminato perchè non rientra nè tra le stopwords nè tra la punteggiatura\n",
    "# - Con questa funzione rimuovo ' da 's e poi rimuovo nuovamente eventuali stopwords.\n",
    "def remove_first_last(tokens, punct, stop):\n",
    "    for i in range(len(tokens)):\n",
    "        for p in punct:\n",
    "            if tokens[i].startswith(p):\n",
    "                tokens[i] = tokens[i][1:]\n",
    "            if tokens[i].endswith(p):\n",
    "                tokens[i] = tokens[i][:-1]\n",
    "    tokens = [t for t in tokens if t not in stop]\n",
    "    return tokens\n",
    "\n",
    "# Remove stopwords and punctuation from the text, tokenize it and lemmatize it\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    stop = []\n",
    "    with open('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/stop_words_FULL.txt', 'r') as f:\n",
    "        stop = f.read().splitlines()\n",
    "    stop = set(stop)\n",
    "    punct = ['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '``', \"''\", '...', '’', '“', '”', '‘']\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop and t not in punct]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = list(set([lemmatizer.lemmatize(t) for t in tokens]))\n",
    "    tokens = remove_first_last(tokens, punct, stop)\n",
    "    return tokens\n",
    "\n",
    "# try preprocessing\n",
    "preprocess(\"This is a test. It's a test of the pre-processing system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing del file di input Nasari e creazione dizionario di vettori Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the Nasari file and creating a dictionary with:\n",
    "# - key: word\n",
    "# - value: dictionary with:\n",
    "#          - key: lemma\n",
    "#          - value: score\n",
    "nasari = {}\n",
    "with open('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/dd-small-nasari-15.txt', 'r') as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "    for line in lines:\n",
    "        line = line.split(';')\n",
    "        tmp = {}\n",
    "        for lemma in line[2:]:\n",
    "            lemma = lemma.split('_')\n",
    "            if len(lemma) > 1:\n",
    "                tmp[lemma[0]] = lemma[1]\n",
    "        nasari[line[1].lower()] = tmp\n",
    "\n",
    "# nasari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvo i documenti di input\n",
    "\n",
    "Rendo ogni documento una lista di paragrafi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save document\n",
    "def save_doc(filename):\n",
    "    doc = []\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = [line.rstrip('\\n') for line in f]\n",
    "        for line in lines:\n",
    "            if '#' not in line and line != '': # remove empty lines and the first line with the link\n",
    "                doc.append(line)\n",
    "    return doc\n",
    "\n",
    "# try save_doc\n",
    "# save_doc('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Individuate the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get title from document, considering the first line\n",
    "def get_title(filename):\n",
    "    doc = save_doc(filename)\n",
    "    return doc[0]\n",
    "\n",
    "# Get topic words from the text checking if they are in the Nasari dictionary\n",
    "def get_topic_words(text):\n",
    "    tokens = preprocess(text)\n",
    "    topic_words = [t for t in tokens if t in nasari.keys()]\n",
    "    return topic_words\n",
    "\n",
    "# Get random paragraph topic words from the document (not the title)\n",
    "def get_random_paragraph(filename):\n",
    "    doc = save_doc(filename)\n",
    "    paragraph = random.choice(doc[1:])\n",
    "    topic_words = get_topic_words(paragraph)\n",
    "    return topic_words, paragraph\n",
    "\n",
    "# get_topic_words(get_title('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt'))\n",
    "# get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_random_paragraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vy/61q2hg6j75vdf08b96dmw_6c0000gn/T/ipykernel_76239/2509608079.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# try create_paragraph_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtopic_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# print(topic_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# print(paragraph)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_random_paragraph' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the context for a document title\n",
    "def create_title_context(filename):\n",
    "    title = get_title(filename)\n",
    "    topic_words = get_topic_words(title)\n",
    "    context_vector = []\n",
    "    for word in topic_words:\n",
    "        context_vector.append(nasari[word])\n",
    "    return context_vector\n",
    "\n",
    "# try create_context\n",
    "# create_title_context('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')\n",
    "\n",
    "# Create the context for a paragraph\n",
    "def create_paragraph_context(paragraph):\n",
    "    topic_words = get_topic_words(paragraph)\n",
    "    context_vector = []\n",
    "    for word in topic_words:\n",
    "        context_vector.append(nasari[word])\n",
    "    return context_vector\n",
    "\n",
    "# try create_paragraph_context\n",
    "for i in range(1):\n",
    "    topic_words, paragraph = get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')\n",
    "    # print(topic_words)\n",
    "    # print(paragraph)\n",
    "    print(create_paragraph_context(paragraph))\n",
    "# create_paragraph_context(get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retain paragraphs whose sentences contain the most salient terms, based on the Weighted Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap: Those who survive often have ongoing muscular and joint pain, liver inflammation, and decreased hearing, and may have continued tiredness, continued weakness, decreased appetite, and difficulty returning to pre-illness weight. Problems with vision may develop.\n",
      "ever here?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vy/61q2hg6j75vdf08b96dmw_6c0000gn/T/ipykernel_73603/348615981.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# try weighted_overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mweighted_overlap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_random_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/vy/61q2hg6j75vdf08b96dmw_6c0000gn/T/ipykernel_73603/348615981.py\u001b[0m in \u001b[0;36mweighted_overlap\u001b[0;34m(v1, v2)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mden\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mget_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This should be the num but since it is to the power of -1 I can put it in the den\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mnum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m \u001b[0;31m# This should be the den but since it is to the power of -1 I can put it in the num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "# Get overlap between a text and a paragraph\n",
    "def get_overlap(text, paragraph):\n",
    "    return text and paragraph\n",
    "\n",
    "# try get_overlap\n",
    "# print(get_overlap('test', 'test'))\n",
    "\n",
    "# Get rank as the position of a lemma in the vector\n",
    "def get_rank(lemma, vector):\n",
    "    for i in range(len(vector)):\n",
    "        if lemma == vector[i]:\n",
    "            return i + 1\n",
    "    print('ever here?')\n",
    "    return None\n",
    "# def get_rank(term, vector):\n",
    "#     for i in range(len(vector)):\n",
    "#         print(term)\n",
    "#         print(vector[i])\n",
    "#         if term in vector[i].keys():\n",
    "#             return i + 1\n",
    "#     return None # If the term is not in the vector\n",
    "\n",
    "# try get_rank\n",
    "# print(get_rank('virus', create_context('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')))\n",
    "\n",
    "# Compute weighted overlap between two vectors\n",
    "def weighted_overlap(v1, v2):\n",
    "    overlap = get_overlap(v1, v2)\n",
    "    print(f'overlap: {overlap}')\n",
    "    if overlap:\n",
    "        i = 0\n",
    "        num = 0\n",
    "        den = 0\n",
    "        for term in overlap:\n",
    "            den += get_rank(term, v1) + get_rank(term, v2) # This should be the num but since it is to the power of -1 I can put it in the den\n",
    "            num += 2 * i # This should be the den but since it is to the power of -1 I can put it in the num\n",
    "            i += 1\n",
    "        return num / den\n",
    "    return 0\n",
    "\n",
    "# try weighted_overlap\n",
    "weighted_overlap(get_title('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt'), get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5587bf738ccec4bb524b17f914a1c07eb93a384af8fabdf9504251bc61fb6171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
