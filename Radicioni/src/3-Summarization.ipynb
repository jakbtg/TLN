{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Automatic Summarization**\n",
    "\n",
    "Algortimo semplice:\n",
    "1. individua argomento del testo come una lista di vettori di Nasari (termine1, score1, termine2, score2, ...)\n",
    "2. crea contesto raccogliendo vettori dei termini trovati al passo prima. Nel caso in cui il titolo sia troppo corto (e quindi poco informativo) posso cercare vettori di elementi che fanno parte delle definizioni dei termini trovati al passo 1\n",
    "3. peso i paragrafi in base alla somma dei pesi dei termini che fanno parte del contesto con la Weighted Overlap. Mantengo solo i paragrafi che hanno un peso maggiore di una soglia\n",
    "\n",
    "Valutazione:\n",
    "- BLEU\n",
    "- ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_paths = ['/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Andy-Warhol.txt',\n",
    "'/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt',\n",
    "'/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Life-indoors.txt',\n",
    "'/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Napoleon-wiki.txt', \n",
    "'/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Trump-wall.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni di utility per il preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pre-processing', 'people', 'system', 'test']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Useful to remove punctuation from first or last char of a token \n",
    "# - Esempio: senza questa funzione \"It's\" diventa \"It\" e \"'s\"\n",
    "# - It viene eliminato perchè è una stopword, mentre 's non viene eliminato perchè non rientra nè tra le stopwords nè tra la punteggiatura\n",
    "# - Con questa funzione rimuovo ' da 's e poi rimuovo nuovamente eventuali stopwords.\n",
    "def remove_first_last(tokens, punct, stop):\n",
    "    for i in range(len(tokens)):\n",
    "        for p in punct:\n",
    "            if tokens[i].startswith(p):\n",
    "                tokens[i] = tokens[i][1:]\n",
    "            if tokens[i].endswith(p):\n",
    "                tokens[i] = tokens[i][:-1]\n",
    "    tokens = [t for t in tokens if t not in stop]\n",
    "    return tokens\n",
    "\n",
    "# Remove stopwords and punctuation from the text, tokenize it and lemmatize it\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    stop = []\n",
    "    with open('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/stop_words_FULL.txt', 'r') as f:\n",
    "        stop = f.read().splitlines()\n",
    "    stop = set(stop)\n",
    "    punct = ['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '``', \"''\", '...', '’', '“', '”', '‘']\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop and t not in punct]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = list(set([lemmatizer.lemmatize(t) for t in tokens]))\n",
    "    tokens = remove_first_last(tokens, punct, stop)\n",
    "    return tokens\n",
    "\n",
    "# try preprocessing\n",
    "preprocess(\"This is a test. It's a test of the pre-processing system. Millions of people are using it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing del file di input Nasari e creazione dizionario di vettori Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the Nasari file and creating a dictionary with:\n",
    "# - key: word\n",
    "# - value: dictionary with:\n",
    "#          - key: lemma\n",
    "#          - value: score\n",
    "nasari = {}\n",
    "with open('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/dd-small-nasari-15.txt', 'r') as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "    for line in lines:\n",
    "        line = line.split(';')\n",
    "        tmp = {}\n",
    "        for lemma in line[2:]:\n",
    "            lemma = lemma.split('_')\n",
    "            if len(lemma) > 1:\n",
    "                tmp[lemma[0]] = lemma[1]\n",
    "        nasari[line[1].lower()] = tmp\n",
    "\n",
    "# nasari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvo i documenti di input\n",
    "\n",
    "Rendo ogni documento una lista di paragrafi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save document\n",
    "def save_doc(filename):\n",
    "    doc = []\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = [line.rstrip('\\n') for line in f]\n",
    "        for line in lines:\n",
    "            if '#' not in line and line != '': # remove empty lines and the first line with the link\n",
    "                doc.append(line)\n",
    "    return doc\n",
    "\n",
    "# try save_doc\n",
    "# save_doc('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Individuate the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get title from document, considering the first line\n",
    "def get_title(filename):\n",
    "    doc = save_doc(filename)\n",
    "    return doc[0]\n",
    "    # return preprocess(doc[0])\n",
    "\n",
    "# Get topic words from the text checking if they are in the Nasari dictionary\n",
    "def get_topic_words(text):\n",
    "    tokens = preprocess(text)\n",
    "    topic_words = [t for t in tokens if t in nasari.keys()]\n",
    "    return topic_words\n",
    "\n",
    "# so far used only for testing\n",
    "# Get random paragraph topic words from the document (not the title)\n",
    "def get_random_paragraph(filename):\n",
    "    doc = save_doc(filename)\n",
    "    paragraph = random.choice(doc[1:])\n",
    "    return paragraph\n",
    "\n",
    "# print(get_title('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Life-indoors.txt'))\n",
    "# print(get_topic_words(get_title('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Life-indoors.txt')))\n",
    "# get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the context for a document title \n",
    "# - It return a list of dictionaries associated to the topic words of the title if they are in the Nasari dictionary\n",
    "def create_context(list):\n",
    "    topic_words = get_topic_words(list)\n",
    "    context_vector = [nasari[word] for word in topic_words]\n",
    "    return context_vector\n",
    "\n",
    "# Create the context for a paragraph\n",
    "# - It returns a list of dictionaries associated to the topic words of the paragraph if they are in the Nasari dictionary\n",
    "def create_paragraph_context(paragraph):\n",
    "    topic = [w for w in paragraph if w in nasari.keys()]\n",
    "    context_vector = [nasari[word] for word in topic]\n",
    "    return context_vector\n",
    "\n",
    "# try create_paragraph_context\n",
    "# for i in range(10):\n",
    "#     topic_words, paragraph = get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Life-indoors.txt')\n",
    "#     print(topic_words)\n",
    "#     # print(paragraph)\n",
    "#     print(create_paragraph_context(paragraph))\n",
    "# create_paragraph_context(get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt')[1])\n",
    "# create_context(get_title('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retain paragraphs whose sentences contain the most salient terms, based on the Weighted Overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementazione della Weighted Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overlap between a text topic words vectors and a paragraph topic words \n",
    "def get_overlap(context, paragraph):\n",
    "    overlap = set()\n",
    "    for w in paragraph:\n",
    "        for dict in context:\n",
    "            if w in dict.keys():\n",
    "                overlap.add(w)\n",
    "    return overlap\n",
    "\n",
    "# Get rank as the position of a lemma in the vector\n",
    "def get_rank(lemma, vector):\n",
    "    min = math.inf\n",
    "    for dict in vector:\n",
    "        i = 1\n",
    "        for key in dict.keys():\n",
    "            if key == lemma:\n",
    "                if i < min:\n",
    "                    min = i\n",
    "            i += 1\n",
    "    return min\n",
    "\n",
    "# Compute weighted overlap between two vectors\n",
    "def weighted_overlap(context, paragraph, par_context):\n",
    "    overlap = get_overlap(context, paragraph)\n",
    "    # print(f'overlap: {overlap}')\n",
    "    # print(f'lenght: {len(overlap)}')\n",
    "    if overlap:\n",
    "        i = 1\n",
    "        num = 0\n",
    "        den = 0\n",
    "        for lemma in overlap:\n",
    "            den += get_rank(lemma, context) + get_rank(lemma, par_context) # This should be the num but since it is to the power of -1 I can put it in the den\n",
    "            num += 2 * i # This should be the den but since it is to the power of -1 I can put it in the num\n",
    "            i += 1\n",
    "            # print(f'lemma: {lemma}, title rank: {get_rank(lemma, context)}, paragraph rank: {get_rank(lemma, par_context)}')\n",
    "            # print(f'total rank: {get_rank(lemma, context) + get_rank(lemma, par_context)}')\n",
    "            # print(f'num: {num}, den: {den}')\n",
    "        return num / den\n",
    "    return 0\n",
    "\n",
    "# for i in range(4):\n",
    "#     par_processed = get_random_paragraph('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Life-indoors.txt')\n",
    "#     title = get_title('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Life-indoors.txt')\n",
    "#     context = create_context(title)\n",
    "#     par_processed = preprocess(par_processed)\n",
    "#     par_context = create_paragraph_context(par_processed)\n",
    "#     print(weighted_overlap(context, par_processed, par_context))\n",
    "#     print('-' * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcolo della Weighted Overlap per ogni paragrafo di un documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andy Warhol: Why the great Pop artist thought ‘Trump is sort of cheap’\n",
      "[1.0, 0, 1.0, 1.0, 0, 0.5, 1.0, 0.0, 0.3333333333333333, 0, 1.0, 0.35294117647058826, 0, 0.0, 0, 0, 0, 0, 1.0]\n",
      "----------------------------------------------------------------------\n",
      "Ebola virus disease\n",
      "[1.5384615384615385, 0.75, 0.75, 1.2, 0.4, 0, 0.8571428571428571, 0.4, 0, 0.4, 0, 0.4, 0, 0.5, 0, 1.5, 0, 0.48, 1.0891089108910892, 0, 1.5384615384615385, 1.0, 1.0909090909090908, 0.5, 1.3333333333333333, 0.0, 1.5]\n",
      "----------------------------------------------------------------------\n",
      "How people around the world are coping with life indoors\n",
      "[0.6, 0.6, 0.6, 0, 0, 0.18181818181818182, 0.4, 0.3333333333333333, 0, 0, 0.3333333333333333, 0.3333333333333333]\n",
      "----------------------------------------------------------------------\n",
      "Napoleone Bonaparte.\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------------------------------------------------------------------\n",
      "The Trump wall, commonly referred to as \"The Wall\", was an expansion of the Mexico–United States barrier during the U.S. presidency of Donald Trump. Throughout his 2016 presidential campaign, Trump called for the construction of a border wall. He said that, if elected, he would \"build the wall and make Mexico pay for it\". Then-Mexican president Enrique Peña Nieto said that Mexico would not pay for the wall.\n",
      "[0.0, 0.0, 0.8571428571428571, 0, 0.0, 0.5, 0.4, 0.0, 0, 0.75, 0.75, 0.75, 0.0, 0.0, 0.0, 0.75, 0, 0, 0.0, 0, 0.7142857142857143, 0, 0.0, 0.0, 0, 0.7142857142857143, 0.5263157894736842, 0.46153846153846156, 0.7894736842105263, 0.5, 0.375, 0, 0, 0.0, 0.0, 0.0, 1.105263157894737, 0, 0, 1.0, 0.0, 0.75, 0.8333333333333334, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.135135135135135, 0.0, 0.0, 0.0, 0.9375, 0.0, 0, 0.0, 0.0, 0, 1.0, 0.5, 0.8108108108108109, 1.6666666666666667]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compute the weighted overlap between a document title and all the paragraphs\n",
    "def weight_doc(filename):\n",
    "    title = get_title(filename)\n",
    "    context = create_context(title)\n",
    "    doc = save_doc(filename)\n",
    "    paragraphs = [preprocess(par) for par in doc[1:]]\n",
    "    par_context = [create_paragraph_context(par) for par in paragraphs]\n",
    "    weighted_overlap_list = [weighted_overlap(context, paragraphs[i], par_context[i]) for i in range(len(paragraphs))]\n",
    "    return weighted_overlap_list\n",
    "\n",
    "for path in doc_paths:\n",
    "    print(get_title(path))\n",
    "    print(weight_doc(path))\n",
    "    print('-' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selezione dei paragrafi migliori\n",
    "\n",
    "Verrà selezionato il 70 - 80 - 90% dei paragrafi con peso maggiore, a seconda della percentuale richiesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many paragraphs to keep given a percentage of the document to be maintained\n",
    "def get_threshold(doc, percentage):\n",
    "    total = len(doc[1:])\n",
    "    threshold = math.ceil(total * percentage)\n",
    "    return threshold\n",
    "\n",
    "# Select the best paragraphs given a threshold\n",
    "def select_best_paragraphs(filename, percentage):\n",
    "    weighted_overlap_list = weight_doc(filename)\n",
    "    doc = save_doc(filename)\n",
    "    paragraphs = doc[1:]\n",
    "    best_paragraphs = [doc[0]]\n",
    "    threshold = get_threshold(doc, percentage)\n",
    "    print(f'threshold with percentage {percentage*100}%: {threshold} out of {len(doc[1:])} paragraphs')\n",
    "    for i in range(int(threshold)):\n",
    "        best_paragraphs.append(paragraphs[weighted_overlap_list.index(max(weighted_overlap_list))])\n",
    "        weighted_overlap_list[weighted_overlap_list.index(max(weighted_overlap_list))] = -1\n",
    "    return best_paragraphs\n",
    "\n",
    "# threshold = get_threshold('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt', 0.9)\n",
    "# select_best_paragraphs('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/docs/Ebola-virus-disease.txt', 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcolo della BLEU e ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andy Warhol: Why the great Pop artist thought ‘Trump is sort of cheap’\n",
      "threshold with percentage 70.0%: 14 out of 19 paragraphs\n",
      "BLEU score with 70.0%: 0.27082337919563315\n",
      "________________________________________\n",
      "threshold with percentage 80.0%: 16 out of 19 paragraphs\n",
      "BLEU score with 80.0%: 0.41911171621149995\n",
      "________________________________________\n",
      "threshold with percentage 90.0%: 18 out of 19 paragraphs\n",
      "BLEU score with 90.0%: 0.5477492206756237\n",
      "________________________________________\n",
      "threshold with percentage 100%: 19 out of 19 paragraphs\n",
      "BLEU score with 100%: 0.6069769786668839\n",
      "________________________________________\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ebola virus disease\n",
      "threshold with percentage 70.0%: 19 out of 27 paragraphs\n",
      "BLEU score with 70.0%: 0.21748054096067912\n",
      "________________________________________\n",
      "threshold with percentage 80.0%: 22 out of 27 paragraphs\n",
      "BLEU score with 80.0%: 0.24260056810319283\n",
      "________________________________________\n",
      "threshold with percentage 90.0%: 25 out of 27 paragraphs\n",
      "BLEU score with 90.0%: 0.26190134312911667\n",
      "________________________________________\n",
      "threshold with percentage 100%: 27 out of 27 paragraphs\n",
      "BLEU score with 100%: 0.2721655269759087\n",
      "________________________________________\n",
      "----------------------------------------------------------------------------------------------------\n",
      "How people around the world are coping with life indoors\n",
      "threshold with percentage 70.0%: 9 out of 12 paragraphs\n",
      "BLEU score with 70.0%: 0.5521733001382539\n",
      "________________________________________\n",
      "threshold with percentage 80.0%: 10 out of 12 paragraphs\n",
      "BLEU score with 80.0%: 0.6458222333114022\n",
      "________________________________________\n",
      "threshold with percentage 90.0%: 11 out of 12 paragraphs\n",
      "BLEU score with 90.0%: 0.6794978325835849\n",
      "________________________________________\n",
      "threshold with percentage 100%: 12 out of 12 paragraphs\n",
      "BLEU score with 100%: 0.7637626158259734\n",
      "________________________________________\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Napoleone Bonaparte.\n",
      "threshold with percentage 70.0%: 12 out of 17 paragraphs\n",
      "BLEU score with 70.0%: 0.6807123983233854\n",
      "________________________________________\n",
      "threshold with percentage 80.0%: 14 out of 17 paragraphs\n",
      "BLEU score with 80.0%: 0.8187307530779819\n",
      "________________________________________\n",
      "threshold with percentage 90.0%: 16 out of 17 paragraphs\n",
      "BLEU score with 90.0%: 0.9428731438548749\n",
      "________________________________________\n",
      "threshold with percentage 100%: 17 out of 17 paragraphs\n",
      "BLEU score with 100%: 1.0\n",
      "________________________________________\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The Trump wall, commonly referred to as \"The Wall\", was an expansion of the Mexico–United States barrier during the U.S. presidency of Donald Trump. Throughout his 2016 presidential campaign, Trump called for the construction of a border wall. He said that, if elected, he would \"build the wall and make Mexico pay for it\". Then-Mexican president Enrique Peña Nieto said that Mexico would not pay for the wall.\n",
      "threshold with percentage 70.0%: 44 out of 62 paragraphs\n",
      "BLEU score with 70.0%: 0.3781115004394478\n",
      "________________________________________\n",
      "threshold with percentage 80.0%: 50 out of 62 paragraphs\n",
      "BLEU score with 80.0%: 0.4742030177888989\n",
      "________________________________________\n",
      "threshold with percentage 90.0%: 56 out of 62 paragraphs\n",
      "BLEU score with 90.0%: 0.5641597761354203\n",
      "________________________________________\n",
      "threshold with percentage 100%: 62 out of 62 paragraphs\n",
      "BLEU score with 100%: 0.6599120175960899\n",
      "________________________________________\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Non ho idea di come funziona\n",
    "# Compute BLEU score for a document\n",
    "def bleu(filename, percentage):\n",
    "    best_paragraphs = select_best_paragraphs(filename, percentage)\n",
    "    doc = save_doc(filename)\n",
    "    bleu = nltk.translate.bleu_score.sentence_bleu([doc], best_paragraphs, weights=(0.5, 0.5))\n",
    "    return bleu\n",
    "\n",
    "# Compute ROUGE score for a document\n",
    "# def rouge(filename, percentage):\n",
    "#     best_paragraphs = select_best_paragraphs(filename, percentage)\n",
    "#     doc = save_doc(filename)\n",
    "#     rouge = rouge_score.rouge_n(best_paragraphs, doc, n=2)\n",
    "#     return rouge\n",
    "\n",
    "for path in doc_paths:\n",
    "    print(get_title(path))\n",
    "    percents = [0.7, 0.8, 0.9, 1]\n",
    "    for p in percents:\n",
    "        print(f'BLEU score with {p*100}%: {bleu(path, p)}')\n",
    "        print('_' * 40)\n",
    "    print('-' * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5587bf738ccec4bb524b17f914a1c07eb93a384af8fabdf9504251bc61fb6171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
