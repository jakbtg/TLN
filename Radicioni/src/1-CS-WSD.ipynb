{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Concept Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wu & Palmer Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth of a synset - number of steps from the synset to the root\n",
    "def depth(s):\n",
    "    if s is None:\n",
    "        return -1\n",
    "    hypernyms = s.hypernyms()\n",
    "    if len(hypernyms) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 + max([depth(h) for h in hypernyms]) # add 1 to get the last step to the synset s\n",
    "\n",
    "# Get all hypernyms of a synset (including itself)\n",
    "def all_hypernyms(s):\n",
    "    hypernyms = [s]\n",
    "    for h in s.hypernyms():\n",
    "        hypernyms.extend(all_hypernyms(h))\n",
    "    return hypernyms\n",
    "\n",
    "# Lowest Common Subsumer\n",
    "def lcs(s1, s2):\n",
    "    hyper1 = all_hypernyms(s1)\n",
    "    hyper2 = all_hypernyms(s2)\n",
    "    if len(hyper1) == 0 or len(hyper2) == 0:\n",
    "        return None\n",
    "    candidates = set(hyper1).intersection(set(hyper2))\n",
    "    if len(candidates) > 0:\n",
    "        return max(candidates, key=depth)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Wu and Palmer similarity\n",
    "def wup_sim(s1, s2):\n",
    "    lcs_syn = lcs(s1, s2)\n",
    "    if lcs_syn is None:\n",
    "        return 0\n",
    "    return 2 * depth(lcs_syn) / (depth(s1) + depth(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest Path Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max depth of all synsets in wordnet\n",
    "def max_depth():\n",
    "    max = 0\n",
    "    for synset in wn.all_synsets():\n",
    "        d = depth(synset)\n",
    "        if d > max:\n",
    "            max = d\n",
    "    return max\n",
    "\n",
    "# max_depth = max_depth() it is always 19, so we can hardcode it\n",
    "max_depth = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find shortest path between two synsets\n",
    "def shortest_path(s1, s2):\n",
    "    lcs_syn = lcs(s1, s2)\n",
    "    if lcs_syn is None:\n",
    "        return max_depth\n",
    "    return depth(s1) + depth(s2) - 2 * depth(lcs_syn)\n",
    "\n",
    "# Shortest path similarity\n",
    "def short_path_sim(s1, s2):\n",
    "    return 2 * max_depth - shortest_path(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leacock & Chodorow Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leacock and Chodorow similarity\n",
    "def leac_sim(s1, s2):\n",
    "    distance = shortest_path(s1, s2)\n",
    "    if distance == 0:\n",
    "        return -np.log(distance + 1 / (2 * max_depth + 1))\n",
    "    return -np.log(distance / (2 * max_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato che l'input in WordSim353 è una coppia di termini, mentre le tre formule utilizzano sensi, per calcolare la similarity fra i 2 termini prendiamo la massima similarity fra tutti i sensi del primo termine e tutti i sensi del secondo termine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max similarity between all synsets of two words, given a similarity function\n",
    "def get_max_similarity(s1, s2, sim_func):\n",
    "    s1_synsets = wn.synsets(s1)\n",
    "    s2_synsets = wn.synsets(s2)\n",
    "    max_sim = 0\n",
    "    # best_s1 = None\n",
    "    # best_s2 = None\n",
    "    for s1_syn in s1_synsets:\n",
    "        for s2_syn in s2_synsets:\n",
    "            sim = sim_func(s1_syn, s2_syn)\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                # best_s1 = s1_syn\n",
    "                # best_s2 = s2_syn\n",
    "    return max_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('/Users/jak/Documents/Uni/TLN/TLN /Radicioni/data/WordSim353.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarities for WordSim353 and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Word 1         Word 2  Human (mean)       wup  shortest   leacock\n",
      "0        love            sex          6.77  0.909091        37  3.637586\n",
      "1       tiger            cat          7.35  0.962963        37  3.637586\n",
      "2       tiger          tiger         10.00  1.000000        38  3.663562\n",
      "3        book          paper          7.46  0.857143        36  2.944439\n",
      "4    computer       keyboard          7.62  0.800000        35  2.538974\n",
      "5    computer       internet          7.58  0.588235        31  1.691676\n",
      "6       plane            car          5.77  0.700000        32  1.845827\n",
      "7       train            car          6.31  0.705882        33  2.028148\n",
      "8   telephone  communication          7.50  0.000000        28  1.335001\n",
      "9  television          radio          6.77  0.900000        36  2.944439\n",
      "\n",
      "Wu and Palmer similarity results\n",
      "Spearman correlation: 0.32205979107230365, p-value: 5.816296751820106e-10\n",
      "Pearson correlation: 0.2756318557372003, p-value: 1.4205427728723012e-07\n",
      "--------------------------------\n",
      "Shortest Path similarity results\n",
      "Spearman correlation: 0.2725348694492164, p-value: 1.9812858428748427e-07\n",
      "Pearson correlation: 0.15778620904840207, p-value: 0.002952296149109044\n",
      "--------------------------------\n",
      "Leacock and Chodorow similarity results\n",
      "Spearman correlation: 0.2725348694492164, p-value: 1.9812858428748427e-07\n",
      "Pearson correlation: 0.3052335447838761, p-value: 4.776364600146794e-09\n"
     ]
    }
   ],
   "source": [
    "# Compute the three similarity measures\n",
    "df['wup'] = df.apply(lambda row: get_max_similarity(row['Word 1'], row['Word 2'], wup_sim), axis=1)\n",
    "df['shortest'] = df.apply(lambda row: get_max_similarity(row['Word 1'], row['Word 2'], short_path_sim), axis=1)\n",
    "df['leacock'] = df.apply(lambda row: get_max_similarity(row['Word 1'], row['Word 2'], leac_sim), axis=1)\n",
    "\n",
    "# Show the dataframe\n",
    "print(df.head(10))\n",
    "\n",
    "# Results for Wu and Palmer similarity\n",
    "print('\\nWu and Palmer similarity results')\n",
    "spearman = stats.spearmanr(df['Human (mean)'], df['wup'])\n",
    "print(f'Spearman correlation: {spearman[0]}, p-value: {spearman[1]}')\n",
    "pearson = stats.pearsonr(df['Human (mean)'], df['wup'])\n",
    "print(f'Pearson correlation: {pearson[0]}, p-value: {pearson[1]}')\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "# Results for Shortest Path similarity\n",
    "print('Shortest Path similarity results')\n",
    "spearman = stats.spearmanr(df['Human (mean)'], df['shortest'])\n",
    "print(f'Spearman correlation: {spearman[0]}, p-value: {spearman[1]}')\n",
    "pearson = stats.pearsonr(df['Human (mean)'], df['shortest'])\n",
    "print(f'Pearson correlation: {pearson[0]}, p-value: {pearson[1]}')\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "# Results for Leacock and Chodorow similarity\n",
    "print('Leacock and Chodorow similarity results')\n",
    "spearman = stats.spearmanr(df['Human (mean)'], df['leacock'])\n",
    "print(f'Spearman correlation: {spearman[0]}, p-value: {spearman[1]}')\n",
    "pearson = stats.pearsonr(df['Human (mean)'], df['leacock'])\n",
    "print(f'Pearson correlation: {pearson[0]}, p-value: {pearson[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Word Sense Disambiguation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesk Algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation from a given sentence\n",
    "def remove_stop_punct(sentence):\n",
    "    stop_words = []\n",
    "    with open('/Users/jak/Documents/Uni/TLN/TLN /Radicioni/data/stop_words_FULL.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            stop_words.append(line.strip())\n",
    "    punct = set(['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '``', \"''\", '...', '’', '“', '”', '—', '–', '‘', '—'])\n",
    "    sentence = [w for w in sentence if not w in stop_words and not w in punct]\n",
    "    return sentence\n",
    "\n",
    "# Get content words for the context\n",
    "def get_context(sentence):\n",
    "    # convert sentence to string\n",
    "    sentence = ''.join(sentence)\n",
    "    # sentence = [w.lower() for w in sentence]\n",
    "    sentence = set(sentence.lower().split())\n",
    "    # lower case\n",
    "    sentence = remove_stop_punct(sentence)\n",
    "    return sentence\n",
    "\n",
    "# Get content words for the signature\n",
    "def get_signature(synset):\n",
    "    signature = set(synset.definition().lower().split())\n",
    "    for example in synset.examples():\n",
    "        signature.update(example.lower().split())\n",
    "    signature = remove_stop_punct(signature)\n",
    "    return signature\n",
    "    \n",
    "# Lesk algorithm\n",
    "def lesk(word, sentence):\n",
    "    best_sense = wn.synsets(word)[0]\n",
    "    # print(f'Starting with sense: {best_sense}')\n",
    "    max_overlap = 0\n",
    "    context = get_context(sentence)\n",
    "    for synset in wn.synsets(word):\n",
    "        signature = get_signature(synset)\n",
    "        overlap = [w for w in context if w in signature]\n",
    "        if len(overlap) > max_overlap:\n",
    "            max_overlap = len(overlap)\n",
    "            best_sense = synset\n",
    "            # print(f'New best sense for {word}: {best_sense}')\n",
    "            # print(f'Signature: {signature}')\n",
    "            # print(f'Context: {context}')\n",
    "            # print(f'Overlap: {overlap}')\n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Arms\n",
      "Sentence: Arms bend at the elbow\n",
      "Best sense: (Synset('arm.n.01'), 'a human limb; technically the part of the superior limb between the shoulder and the elbow but commonly used to refer to the whole superior limb')\n",
      "--------------------------------\n",
      "Word: arms\n",
      "Sentence: Germany sells arms to Saudi Arabia\n",
      "Best sense: (Synset('arm.v.02'), 'supply with arms')\n",
      "--------------------------------\n",
      "Word: key\n",
      "Sentence: The key broke in the lock\n",
      "Best sense: (Synset('key.n.01'), \"metal device shaped in such a way that when it is inserted into the appropriate lock the lock's mechanism can be rotated\")\n",
      "--------------------------------\n",
      "Word: key\n",
      "Sentence: The key problem was not one of quality but of quantity\n",
      "Best sense: (Synset('key.n.02'), 'something crucial for explaining')\n",
      "--------------------------------\n",
      "Word: solution\n",
      "Sentence: Work out the solution in your head\n",
      "Best sense: (Synset('solution.n.01'), 'a homogeneous mixture of two or more substances; frequently (but not necessarily) a liquid solution')\n",
      "--------------------------------\n",
      "Word: solution\n",
      "Sentence: Heat the solution to 75° Celsius\n",
      "Best sense: (Synset('solution.n.01'), 'a homogeneous mixture of two or more substances; frequently (but not necessarily) a liquid solution')\n",
      "--------------------------------\n",
      "Word: ashes\n",
      "Sentence: The house was burnt to ashes while the owner returned\n",
      "Best sense: (Synset('ash.v.01'), 'convert into ashes')\n",
      "--------------------------------\n",
      "Word: ash\n",
      "Sentence: This table is made of ash wood\n",
      "Best sense: (Synset('ash.n.03'), 'strong elastic wood of any of various ash trees; used for furniture and tool handles and sporting goods such as baseball bats')\n",
      "--------------------------------\n",
      "Word: lunch\n",
      "Sentence: The lunch with her boss took longer than she expected\n",
      "Best sense: (Synset('lunch.n.01'), 'a midday meal')\n",
      "--------------------------------\n",
      "Word: lunch\n",
      "Sentence: She packed her lunch in her purse\n",
      "Best sense: (Synset('lunch.n.01'), 'a midday meal')\n",
      "--------------------------------\n",
      "Word: classification\n",
      "Sentence: The classification of the genetic data took two years\n",
      "Best sense: (Synset('categorization.n.03'), 'the act of distributing things into classes or categories of the same type')\n",
      "--------------------------------\n",
      "Word: classification\n",
      "Sentence: The journal Science published the classification this month\n",
      "Best sense: (Synset('categorization.n.03'), 'the act of distributing things into classes or categories of the same type')\n",
      "--------------------------------\n",
      "Word: wood\n",
      "Sentence: His cottage is near a small wood\n",
      "Best sense: (Synset('wood.n.01'), 'the hard fibrous lignified substance under the bark of trees')\n",
      "--------------------------------\n",
      "Word: wood\n",
      "Sentence: The statue was made out of a block of wood\n",
      "Best sense: (Synset('wood.n.01'), 'the hard fibrous lignified substance under the bark of trees')\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Try the algorithm on the file sentences.txt\n",
    "with open('/Users/jak/Documents/Uni/TLN/TLN /Radicioni/data/sentences.txt', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i > 2 and i < 17:\n",
    "            line = line.strip()[2:-1]\n",
    "            # find word between **\n",
    "            word = re.search(r'\\*\\*(.*?)\\*\\*', line).group(1) # TODO: understand better the regex\n",
    "            sentence = line.replace(f'**{word}**', word)\n",
    "            sense = lesk(word, sentence)\n",
    "            print(f'Word: {word}')\n",
    "            print(f'Sentence: {sentence}')\n",
    "            print(f'Best sense: {sense, sense.definition()}')\n",
    "            print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting 50 sentences from SemCor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import semcor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: [Tree(Lemma('group.n.01.group'), [Tree('NE', [Tree('NNP', ['Fulton', 'County', 'Grand', 'Jury'])])]), Tree(Lemma('friday.n.01.Friday'), [Tree('NN', ['Friday'])]), Tree(Lemma('probe.n.01.investigation'), [Tree('NN', ['investigation'])]), Tree(Lemma('atlanta.n.01.Atlanta'), [Tree('NN', ['Atlanta'])]), Tree(Lemma('primary.n.01.primary_election'), [Tree('NN', ['primary', 'election'])]), Tree(Lemma('evidence.n.01.evidence'), [Tree('NN', ['evidence'])]), Tree(Lemma('abnormality.n.04.irregularity'), [Tree('NN', ['irregularities'])])]\n",
      "Choosen noun: (Lemma('evidence.n.01.evidence') (NN evidence))\n"
     ]
    }
   ],
   "source": [
    "sentences = semcor.sents()\n",
    "tagged_sentences = semcor.tagged_sents(tag='both') # tag='both' to get both the POS tag and the synset from the gold corpus, in orderd to compare with the results of my algorithm\n",
    "\n",
    "# Extract the first 50 sentences from the corpus\n",
    "first_sentences = sentences[:50]\n",
    "first_tagged_sentences = tagged_sentences[:50]\n",
    "# print(first_tagged_sentences[0])\n",
    "\n",
    "# Get a random noun given a tagged sentence\n",
    "def get_noun(tagged_sentence):\n",
    "    nouns = []\n",
    "    for i in range(len(tagged_sentence)):\n",
    "        # print(tagged_sentence[i])\n",
    "        sent = str(tagged_sentence[i])\n",
    "        # print(sent)\n",
    "        if 'Lemma' in sent and 'NN' in sent:\n",
    "            # print('found')\n",
    "            nouns.append(tagged_sentence[i])\n",
    "    print(f'Nouns: {nouns}')\n",
    "    if len(nouns) > 0:\n",
    "        return random.choice(nouns)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "noun = get_noun(first_tagged_sentences[0])\n",
    "print(f'Choosen noun: {noun}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute '_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vy/61q2hg6j75vdf08b96dmw_6c0000gn/T/ipykernel_42463/1166467973.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy: {get_accuracy()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/vy/61q2hg6j75vdf08b96dmw_6c0000gn/T/ipykernel_42463/1166467973.py\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_tagged_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_noun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vy/61q2hg6j75vdf08b96dmw_6c0000gn/T/ipykernel_42463/808480087.py\u001b[0m in \u001b[0;36mget_noun\u001b[0;34m(tagged_sentence)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Get a random noun given a tagged sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_noun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnouns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_sentence\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vy/61q2hg6j75vdf08b96dmw_6c0000gn/T/ipykernel_42463/808480087.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Get a random noun given a tagged sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_noun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnouns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_sentence\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '_name'"
     ]
    }
   ],
   "source": [
    "# Compare the accuracy of my algorithm with the sense of the word in the corpus\n",
    "def compare_senses(word, sentence):\n",
    "    my_sense = lesk(word, sentence)\n",
    "    corpus_sense = wn.synsets(word)[0]\n",
    "    for synset in wn.synsets(word):\n",
    "        if synset in semcor.senses(word):\n",
    "            corpus_sense = synset\n",
    "            break\n",
    "    return my_sense == corpus_sense\n",
    "\n",
    "# Get the accuracy of the algorithm\n",
    "def get_accuracy():\n",
    "    correct = 0\n",
    "    for i, sentence in enumerate(first_tagged_sentences):\n",
    "        word = get_noun(sentence)\n",
    "        if word is None:\n",
    "            continue\n",
    "        if compare_senses(word, first_sentences[i]):\n",
    "            correct += 1\n",
    "    return correct / len(first_sentences)\n",
    "\n",
    "print(f'Accuracy: {get_accuracy()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5587bf738ccec4bb524b17f914a1c07eb93a384af8fabdf9504251bc61fb6171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
