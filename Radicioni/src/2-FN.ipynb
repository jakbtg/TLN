{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mapping of frames in WordNet synsets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frames estratti con la funzione getFrameSetForStudent() per il cognome Grandi:\n",
    "| ID | Frame |\n",
    "| --- | --- |\n",
    "| 2658 | Suicide_attack |\n",
    "| 1633 | Board_vehicle |\n",
    "| 1260 | Simple_name |\n",
    "| 1871 | Access_scenario |\n",
    "| 652 | Eclipse |\n",
    "\n",
    "Dato che: \n",
    " - Suicide_attack\n",
    " - Board_vehicle \n",
    " - Simple_name \n",
    " - Access_scenario \n",
    "\n",
    "non sono presenti in WordNet, sono stati scelti rispettivamente:\n",
    "- Suicide\n",
    "- Vehicle\n",
    "- Name\n",
    "- Access\n",
    "\n",
    "In pratica dovrò: associare dei synset al frame name, frame elements e LUs presi dai 5 del frameset.\n",
    "Considerando come contesto di disambiguazione la frame name definition, frame element definition e LU definition, rispettivamente.\n",
    "Come contesto dei sensi considero le varie definizioni del termine principale del frame name, la gloss, esempi e iponimi e iperonimi. E lo stesso per FE e LU\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame ids for student Grandi\n",
    "frame_ids = [2658, 1633, 1260, 1871, 652]\n",
    "\n",
    "# Simplified frame names\n",
    "simplified_names = ['Suicide', 'Vehicle', 'Name', 'Access', 'Eclipse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo manualmente le annotazioni\n",
    "\n",
    "Per ogni frame assegno un synset a frame name, frame elements e le prime 10 lexical units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with: \n",
    "# - key = frame \n",
    "# - value = dictionary with: \n",
    "#           - key = word (Frame name, FE, LU\n",
    "#           - value = \n",
    "annotations = {\n",
    "    'Suicide_attack': {\n",
    "        'Suicide': 'suicide.n.01',\n",
    "        'Victim': 'victim.n.01',\n",
    "        'Weapon': 'weapon.n.01',\n",
    "        'Assailant': 'attacker.n.01',\n",
    "        'Manner': 'manner.n.01',\n",
    "        'Means': 'means.n.01',\n",
    "        'Place': 'topographic_point.n.01',\n",
    "        'Purpose': 'purpose.n.01',\n",
    "        'Time': 'fourth_dimension.n.01',\n",
    "        'Source': 'beginning.n.04',\n",
    "        'Depictive': 'delineative.s.01',\n",
    "        'Re-encoding': None,\n",
    "        'Result': 'consequence.n.01',\n",
    "        'Containing_event': None,\n",
    "        'Explanation': 'explanation.n.01',\n",
    "        'Particular_iteration': None,\n",
    "        'Circumstances': 'circumstance.n.01',\n",
    "        'Path': 'path.n.03',\n",
    "        'suicide attack.n': 'suicide_bombing.n.01',\n",
    "        'suicide bombing.n': 'suicide_bombing.n.01',\n",
    "        'kamikaze [attack].n': 'kamikaze.n.02',\n",
    "        'suicide bomber.n': 'suicide_bomber.n.01',\n",
    "        'suicide attacker.n': 'suicide_bomber.n.01',\n",
    "        'kamikaze [person].n': 'kamikaze.n.02'\n",
    "    },\n",
    "    'Board_vehicle': {\n",
    "        'Vehicle': 'vehicle.n.01',\n",
    "        'Traveller': 'traveler.n.01',\n",
    "        'Vehicle': 'vehicle.n.01',\n",
    "        'Source': 'beinning.n.04',\n",
    "        'Manner': 'manner.n.01',\n",
    "        'Path': 'path.n.03',\n",
    "        'Duration_of_final_state': 'duration.n.01',\n",
    "        'Depictive': 'delineative.s.01',\n",
    "        'Time': 'fourth_dimension.n.01',\n",
    "        'Cotheme': None,\n",
    "        'Purpose': 'purpose.n.01',\n",
    "        'Place': 'topographic_point.n.01',\n",
    "        'Circumstances': 'circumstance.n.01',\n",
    "        'board.v': 'board.v.01',\n",
    "        'get.v': 'board.v.01',\n",
    "        'embark.v': 'embark.v.01',\n",
    "        'emplane.v': 'emplane.v.01',\n",
    "        'entrain.v': 'entrain.v.01',\n",
    "        'mount.v': 'hop_on.v.01',\n",
    "        'hop.v': 'hop.v.03',\n",
    "        'embarkation.n': 'boarding.n.01',\n",
    "        'embarkment.n': 'boarding.n.01',\n",
    "        'embus.v': None\n",
    "    },\n",
    "    'Simple_name': {\n",
    "        'Name': 'name.n.01',\n",
    "        'Term': 'term.n.01',\n",
    "        'Entity': 'entity.n.01',\n",
    "        'Speaker': 'speaker.n.01',\n",
    "        'term.n': 'term.n.01',\n",
    "        'word.n': 'word.n.01'\n",
    "    },\n",
    "    'Access_scenario': {\n",
    "        'Access': 'access.n.06',\n",
    "        'Theme': 'subject.n.01',\n",
    "        'Useful_location': 'location.n.01',\n",
    "        'Barrier': 'barrier.n.01'\n",
    "    },\n",
    "    'Eclipse': {\n",
    "        'Eclipse': 'eclipse.n.01',\n",
    "        'Obstruction': 'obstruction.n.01',\n",
    "        'Eclipsed': 'eclipse.v.02',\n",
    "        'Vantage_point': 'vantage_point.n.01',\n",
    "        'Degree': 'degree.n.01',\n",
    "        'Subregion': 'region.n.01',\n",
    "        'hide.v': 'hide.v.01',\n",
    "        'eclipse.v': 'eclipse.v.02',\n",
    "        'mask.v': 'mask.v.05',\n",
    "        'cover.v': 'cover.v.01',\n",
    "        'obscure.v': 'obscure.v.01',\n",
    "        'screen.v': 'screen.v.05',\n",
    "        'veil.v': 'veil.v.01',\n",
    "        'hidden.a': 'hide.v.01',\n",
    "        'blot out.v': 'obscure.v.05',\n",
    "        'obscured.a': 'obscure.n.01'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni di utility per il preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and punctuation from a given sentence\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = []\n",
    "    with open('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/stop_words_FULL.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            stop_words.append(line.strip())\n",
    "    sentence = [w for w in sentence if w not in stop_words]\n",
    "    return sentence\n",
    "\n",
    "# Remove punctuation from a given list of words\n",
    "def remove_punctuation(list):\n",
    "    final_list = []\n",
    "    punct = set(['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '``', \"''\", '...', '’', '“', '”']) # Keeping '-' between words\n",
    "    for word in list:\n",
    "        for c in word:\n",
    "            if c in punct:\n",
    "                word = word.replace(c, '')\n",
    "            if c == '_':\n",
    "                word = word.replace(c, ' ')\n",
    "        final_list.append(word)\n",
    "    return final_list\n",
    "\n",
    "# Lemmatize a given list of words\n",
    "def lemmatize(list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_set = set()\n",
    "    for word in list:\n",
    "        final_set.add(lemmatizer.lemmatize(word))\n",
    "    return final_set\n",
    "\n",
    "# Full preprocessing of a given list of words\n",
    "def preprocess(list):\n",
    "    list = remove_stopwords(list) # Don't get why if I remove stopwords even here I get less words\n",
    "    list = remove_punctuation(list)\n",
    "    list = lemmatize(list)\n",
    "    list = remove_stopwords(list)\n",
    "    list = [x for x in list if x] # Remove empty strings\n",
    "    return set(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del contesto dei sensi\n",
    "\n",
    "Creo una lista di content words per un dato synset, cercando all'interno di:\n",
    "- nome del synset\n",
    "- definizione del synset\n",
    "- esempi del synset\n",
    "- definizioni di iponimi e iperonimi\n",
    "- esempi di iponimi e iperonimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context for a given synset\n",
    "def get_context(synset):\n",
    "    context = set(synset.name().split('.')[0].split('_')) \n",
    "    context.update(synset.definition().lower().split())\n",
    "    for example in synset.examples():\n",
    "        context.update(example.lower().split())\n",
    "    for hypo in synset.hyponyms():\n",
    "        context.update(hypo.name().split('.')[0].split('_'))\n",
    "        context.update(hypo.definition().lower().split())\n",
    "        for example in hypo.examples():\n",
    "            context.update(example.lower().split())\n",
    "    for hyper in synset.hypernyms():\n",
    "        context.update(hyper.name().split('.')[0].split('_'))\n",
    "        context.update(hyper.definition().lower().split())\n",
    "        for example in hyper.examples():\n",
    "            context.update(example.lower().split())\n",
    "    context = preprocess(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del contesto del frame\n",
    "\n",
    "Creo una lista di content words per un dato frame, cercando all'interno di:\n",
    "- Frame name\n",
    "- Frame definition\n",
    "- Frame Element name\n",
    "- Frame definition\n",
    "- Lexical Unit name\n",
    "- Lexical Unit definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context for a given frame\n",
    "def get_frame_context(frame):\n",
    "    context = set(frame.definition.lower().split())\n",
    "    context.update(frame.name.lower().split())\n",
    "    for fe in frame.FE:\n",
    "        context.update(frame.FE[fe].definition.lower().split())\n",
    "        context.update(fe.lower().split())\n",
    "    for lu in frame.lexUnit:\n",
    "        context.update(frame.lexUnit[lu].lexemes[0].name.lower().split())\n",
    "        context.update(frame.lexUnit[lu].definition.lower().split())\n",
    "    context = preprocess(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del contesto del frame element\n",
    "\n",
    "Creo una lista di content words per un dato frame element, cercando all'interno di:\n",
    "- Frame Element name\n",
    "- Frame Element definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context for a given frame element\n",
    "def get_fe_context(frame_element):\n",
    "    context = set(frame_element.definition.lower().split())\n",
    "    context.update(frame_element.name.lower().split())\n",
    "    context = preprocess(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del contesto del lexical unit\n",
    "\n",
    "Creo una lista di content words per un dato lexical unit, cercando all'interno di:\n",
    "- Lexical Unit name\n",
    "- Lexical Unit definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context for a given lexical unit\n",
    "def get_lu_context(lexical_unit):\n",
    "    context = set(lexical_unit.lexemes[0].name.lower().split())\n",
    "    context.update(lexical_unit.definition.lower().split())\n",
    "    context = preprocess(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping - Approccio bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best synset for a given word and context\n",
    "def find_best_syn(word, context):\n",
    "    best = 0\n",
    "    best_syn = None\n",
    "    for synset in wn.synsets(word):\n",
    "        syn_context = get_context(synset)\n",
    "        score = len(syn_context.intersection(context)) + 1 # + 1 According to the paper\n",
    "        if score > best:\n",
    "            best = score\n",
    "            best_syn = synset\n",
    "    return best_syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disambiguazione dei synset di: frame names, FEs e LUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRAME: Suicide_attack\n",
      "\tName: Suicide - Synset('suicide.n.01')\n",
      "\tFE: Victim - Synset('victim.n.01')\n",
      "\tFE: Weapon - Synset('weapon.n.01')\n",
      "\tFE: Assailant - Synset('attacker.n.01')\n",
      "\tFE: Manner - Synset('manner.n.01')\n",
      "\tFE: Means - Synset('means.n.01')\n",
      "\tFE: Place - Synset('topographic_point.n.01')\n",
      "\tFE: Purpose - Synset('purpose.n.01')\n",
      "\tFE: Time - Synset('time.n.01')\n",
      "\tFE: Source - Synset('beginning.n.04')\n",
      "\tFE: Depictive - Synset('delineative.s.01')\n",
      "\tFE: Re-encoding - None\n",
      "\tFE: Result - Synset('consequence.n.01')\n",
      "\tFE: Containing_event - None\n",
      "\tFE: Explanation - Synset('explanation.n.01')\n",
      "\tFE: Particular_iteration - None\n",
      "\tFE: Circumstances - Synset('circumstance.n.01')\n",
      "\tFE: Path - Synset('path.n.03')\n",
      "\tLU: suicide attack.n - Synset('suicide.n.01')\n",
      "\tLU: suicide bombing.n - Synset('suicide.n.02')\n",
      "\tLU: kamikaze [attack].n - Synset('kamikaze.n.01')\n",
      "\tLU: suicide bomber.n - Synset('suicide.n.01')\n",
      "\tLU: suicide attacker.n - Synset('suicide.n.02')\n",
      "\tLU: kamikaze [person].n - Synset('kamikaze.n.01')\n",
      "Total words for frame Suicide_attack: 24\n",
      "----------------------------------------------------------------------\n",
      "FRAME: Board_vehicle\n",
      "\tName: Vehicle - Synset('vehicle.n.01')\n",
      "\tFE: Traveller - Synset('traveler.n.01')\n",
      "\tFE: Vehicle - Synset('vehicle.n.01')\n",
      "\tFE: Source - Synset('generator.n.03')\n",
      "\tFE: Manner - Synset('manner.n.01')\n",
      "\tFE: Path - Synset('way.n.05')\n",
      "\tFE: Duration_of_final_state - None\n",
      "\tFE: Depictive - Synset('delineative.s.01')\n",
      "\tFE: Time - Synset('time.n.01')\n",
      "\tFE: Cotheme - None\n",
      "\tFE: Purpose - Synset('purpose.n.01')\n",
      "\tFE: Place - Synset('put.v.01')\n",
      "\tFE: Circumstances - Synset('circumstance.n.01')\n",
      "\tLU: board.v - Synset('board.v.01')\n",
      "\tLU: get.v - Synset('get.v.01')\n",
      "\tLU: embark.v - Synset('embark.v.01')\n",
      "\tLU: emplane.v - Synset('emplane.v.01')\n",
      "\tLU: entrain.v - Synset('entrain.v.01')\n",
      "\tLU: mount.v - Synset('mount.n.04')\n",
      "\tLU: hop.v - Synset('hop.n.01')\n",
      "\tLU: embarkation.n - Synset('boarding.n.01')\n",
      "\tLU: embarkment.n - Synset('boarding.n.01')\n",
      "\tLU: embus.v - None\n",
      "Total words for frame Board_vehicle: 23\n",
      "----------------------------------------------------------------------\n",
      "FRAME: Simple_name\n",
      "\tName: Name - Synset('name.n.01')\n",
      "\tFE: Term - Synset('term.n.01')\n",
      "\tFE: Entity - Synset('entity.n.01')\n",
      "\tFE: Speaker - Synset('speaker.n.01')\n",
      "\tLU: term.n - Synset('term.n.01')\n",
      "\tLU: word.n - Synset('word.n.01')\n",
      "Total words for frame Simple_name: 6\n",
      "----------------------------------------------------------------------\n",
      "FRAME: Access_scenario\n",
      "\tName: Access - Synset('access.n.03')\n",
      "\tFE: Theme - Synset('subject.n.01')\n",
      "\tFE: Useful_location - None\n",
      "\tFE: Barrier - Synset('barrier.n.01')\n",
      "Total words for frame Access_scenario: 4\n",
      "----------------------------------------------------------------------\n",
      "FRAME: Eclipse\n",
      "\tName: Eclipse - Synset('eclipse.n.01')\n",
      "\tFE: Obstruction - Synset('obstruction.n.01')\n",
      "\tFE: Eclipsed - Synset('overshadow.v.01')\n",
      "\tFE: Vantage_point - Synset('vantage_point.n.01')\n",
      "\tFE: Degree - Synset('degree.n.02')\n",
      "\tFE: Subregion - None\n",
      "\tLU: hide.v - Synset('hide.v.01')\n",
      "\tLU: eclipse.v - Synset('eclipse.n.01')\n",
      "\tLU: mask.v - Synset('mask.n.01')\n",
      "\tLU: cover.v - Synset('cover.v.01')\n",
      "\tLU: obscure.v - Synset('obscure.v.01')\n",
      "\tLU: screen.v - Synset('screen.n.04')\n",
      "\tLU: veil.v - Synset('veil.v.01')\n",
      "\tLU: hidden.a - Synset('hide.v.01')\n",
      "\tLU: blot out.v - Synset('smudge.n.02')\n",
      "\tLU: obscured.a - Synset('obscure.v.01')\n",
      "Total words for frame Eclipse: 16\n",
      "----------------------------------------------------------------------\n",
      "Total words for all frames: 73\n"
     ]
    }
   ],
   "source": [
    "# Find best synset for:\n",
    "# - frame (with the simplified name)\n",
    "#   - frame elements\n",
    "#   - lexical units\n",
    "def disambiguate():\n",
    "    total = 0\n",
    "    dict = {}\n",
    "    for frame_id in frame_ids:\n",
    "        frame = fn.frame_by_id(frame_id)\n",
    "        dict[frame.name] = {}\n",
    "\n",
    "        # Frame\n",
    "        word = simplified_names[frame_ids.index(frame_id)]\n",
    "        frame_context = get_frame_context(frame)\n",
    "        best_syn = find_best_syn(word, frame_context)\n",
    "        dict[frame.name][word] = best_syn\n",
    "        print(f'FRAME: {frame.name}\\n\\tName: {word} - {best_syn}')\n",
    "        \n",
    "        # Frame elements\n",
    "        for fe in frame.FE:\n",
    "            fe_context = get_fe_context(frame.FE[fe])\n",
    "            best_syn = find_best_syn(fe, fe_context)\n",
    "            dict[frame.name][fe] = best_syn\n",
    "            print(f'\\tFE: {fe} - {best_syn}')\n",
    "            \n",
    "        # Lexical units\n",
    "        count = 0\n",
    "        for lu in frame.lexUnit:\n",
    "            # Disambiguate only the first 10 lexical units -- too many to manually annotate\n",
    "            if count >= 10:\n",
    "                break\n",
    "            count += 1\n",
    "            lu_context = get_lu_context(frame.lexUnit[lu])\n",
    "            best_syn = find_best_syn(frame.lexUnit[lu].lexemes[0].name, lu_context)\n",
    "            dict[frame.name][lu] = best_syn\n",
    "            print(f'\\tLU: {lu} - {best_syn}')\n",
    "        \n",
    "        # Total words to disambiguate = 1 frame name + len(frame elements) + count (max 10 lexical units)\n",
    "        print(f'Total words for frame {frame.name}: {len(frame.FE) + count + 1}')\n",
    "        print('-' * 70)\n",
    "        total += len(frame.FE) + count + 1\n",
    "\n",
    "    print(f'Total words for all frames: {total}')\n",
    "    return dict\n",
    "\n",
    "result = disambiguate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutare i risultati con le annotations manuali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.27777777777779%\n",
      "Correct: 47\n",
      "Total: 72\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of the disambiguation using the manually annotated data\n",
    "def compute_accuracy():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for key in result:\n",
    "        for word in result[key]:\n",
    "            if word in annotations[key]:\n",
    "                total += 1\n",
    "                if result[key][word] is None and annotations[key][word] is None:\n",
    "                    correct += 1 # I will consider None as correct if the annotation is None\n",
    "                elif result[key][word] is not None:\n",
    "                    if result[key][word].name() == annotations[key][word]:\n",
    "                        correct += 1\n",
    "    return correct, total\n",
    "\n",
    "correct, total = compute_accuracy()\n",
    "print(f'Accuracy: {(correct / total) * 100}%')\n",
    "print(f'Correct: {correct}')\n",
    "print(f'Total: {total}') \n",
    "# Total words is 1 less than the total words for all frames because there is a duplicate in the frame Board_vehicle:\n",
    "# - Simplified frame name: Vehicle\n",
    "# - Second frame element: Vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in a csv file in the output folder\n",
    "def save_result():\n",
    "    header = ['Word', 'Found', 'Gold']\n",
    "    with open('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/output/disambiguation-results.csv', 'w', newline='') as f:\n",
    "        for key in result:\n",
    "            cap_title = key.upper()\n",
    "            f.write(f'{cap_title}\\n')\n",
    "            for word in result[key]:\n",
    "                cap_word = word.upper()\n",
    "                if result[key][word] is not None:\n",
    "                    synset = result[key][word].name()\n",
    "                else:\n",
    "                    synset = 'None'\n",
    "                if synset == annotations[key][word] or (synset == 'None' and annotations[key][word] is None):\n",
    "                    check = '✅'\n",
    "                else:\n",
    "                    check = '❌'\n",
    "                f.write(f'\\t - {cap_word},\\t Found: {synset},\\t Gold: {annotations[key][word]},\\t {check}\\n')\n",
    "                    \n",
    "save_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5587bf738ccec4bb524b17f914a1c07eb93a384af8fabdf9504251bc61fb6171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
