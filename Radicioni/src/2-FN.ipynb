{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mapping of frames in WordNet synsets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frames estratti con la funzione getFrameSetForStudent() per il cognome Grandi:\n",
    "| ID | Frame |\n",
    "| --- | --- |\n",
    "| 2658 | Suicide_attack |\n",
    "| 1633 | Board_vehicle |\n",
    "| 1260 | Simple_name |\n",
    "| 1871 | Access_scenario |\n",
    "| 652 | Eclipse |\n",
    "\n",
    "Dato che: \n",
    " - Suicide_attack\n",
    " - Board_vehicle \n",
    " - Simple_name \n",
    " - Access_scenario \n",
    "\n",
    "non sono presenti in WordNet, sono stati scelti rispettivamente:\n",
    "- Suicide\n",
    "- Vehicle\n",
    "- Name\n",
    "- Access\n",
    "\n",
    "In pratica dovrò: associare dei synset al frame name, frame elements e LUs presi dai 5 del frameset.\n",
    "Considerando come contesto di disambiguazione la frame name definition, frame element definition e LU definition, rispettivamente.\n",
    "Come contesto dei sensi considero le varie definizioni del termine principale del frame name, la gloss, esempi e iponimi e iperonimi. E lo stesso per FE e LU\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame ids for student Grandi\n",
    "frame_ids = [2658, 1633, 1260, 1871, 652]\n",
    "\n",
    "# Simplified frame names\n",
    "simplified_names = ['Suicide', 'Vehicle', 'Name', 'Access', 'Eclipse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo manualmente le annotazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with: \n",
    "# - key = frame \n",
    "# - value = dictionary with: \n",
    "#           - key = word (Frame name, FE, LU\n",
    "#           - value = \n",
    "annotations = {\n",
    "    'Suicide_attack': {\n",
    "        'Suicide': 'suicide.n.01',\n",
    "        'Victim': 'victim.n.01',\n",
    "        'Weapon': 'weapon.n.01',\n",
    "        'Assailant': 'attacker.n.01',\n",
    "        'Manner': 'manner.n.01',\n",
    "        'Means': 'means.n.01',\n",
    "        'Place': 'topographic_point.n.01',\n",
    "        'Purpose': 'purpose.n.01',\n",
    "        'Time': 'fourth_dimension.n.01',\n",
    "        'Source': 'beginning.n.04',\n",
    "        'Depictive': 'delineative.s.01',\n",
    "        'Re-encoding': None,\n",
    "        'Result': 'consequence.n.01',\n",
    "        'Containing_event': None,\n",
    "        'Explanation': 'explanation.n.01',\n",
    "        'Particular_iteration': None,\n",
    "        'Circumstances': 'circumstance.n.01',\n",
    "        'Path': 'path.n.03',\n",
    "        'suicide attack.n': 'suicide_bombing.n.01',\n",
    "        'suicide bombing.n': 'suicide_bombing.n.01',\n",
    "        'kamikaze [attack].n': 'kamikaze.n.02',\n",
    "        'suicide bomber.n': 'suicide_bomber.n.01',\n",
    "        'suicide attacker.n': 'suicide_bomber.n.01',\n",
    "        'kamikaze [person].n': 'kamikaze.n.02'\n",
    "    },\n",
    "    'Board_vehicle': {\n",
    "        'Vehicle': 'vehicle.n.01',\n",
    "        'Traveller': 'traveler.n.01',\n",
    "        'Vehicle': 'vehicle.n.01',\n",
    "        'Source': 'beinning.n.04',\n",
    "        'Manner': 'manner.n.01',\n",
    "        'Path': 'path.n.03',\n",
    "        'Duration_of_final_state': 'duration.n.01',\n",
    "        'Depictive': 'delineative.s.01',\n",
    "        'Time': 'fourth_dimension.n.01',\n",
    "        'Cotheme': None,\n",
    "        'Purpose': 'purpose.n.01',\n",
    "        'Place': 'topographic_point.n.01',\n",
    "        'Circumstances': 'circumstance.n.01',\n",
    "        'board.v': 'board.v.01',\n",
    "        'get.v': 'board.v.01',\n",
    "        'embark.v': 'embark.v.01',\n",
    "        'emplane.v': 'emplane.v.01',\n",
    "        'entrain.v': 'entrain.v.01',\n",
    "        'mount.v': 'hop_on.v.01',\n",
    "        'hop.v': 'hop.v.03',\n",
    "        'embarkation.n': 'boarding.n.01',\n",
    "        'embarkment.n': 'boarding.n.01',\n",
    "        'embus.v': None\n",
    "    },\n",
    "    'Simple_name': {\n",
    "        'Name': 'name.n.01',\n",
    "        'Term': 'term.n.01',\n",
    "        'Entity': 'entity.n.01',\n",
    "        'Speaker': 'speaker.n.01',\n",
    "        'term.n': 'term.n.01',\n",
    "        'word.n': 'word.n.01'\n",
    "    },\n",
    "    'Access_scenario': {\n",
    "        'Access': 'access.n.06',\n",
    "        'Theme': 'subject.n.01',\n",
    "        'Useful_location': 'location.n.01',\n",
    "        'Barrier': 'barrier.n.01'\n",
    "    },\n",
    "    'Eclipse': {\n",
    "        'Eclipse': 'eclipse.n.01',\n",
    "        'Obstruction': 'obstruction.n.01',\n",
    "        'Eclipsed': 'eclipse.v.02',\n",
    "        'Vantage_point': 'vantage_point.n.01',\n",
    "        'Degree': 'degree.n.01',\n",
    "        'Subregion': 'region.n.01',\n",
    "        'hide.v': 'hide.v.01',\n",
    "        'eclipse.v': 'eclipse.v.02',\n",
    "        'mask.v': 'mask.v.05',\n",
    "        'cover.v': 'cover.v.01',\n",
    "        'obscure.v': 'obscure.v.01',\n",
    "        'screen.v': 'screen.v.05',\n",
    "        'veil.v': 'veil.v.01',\n",
    "        'hidden.a': 'hide.v.01',\n",
    "        'blot out.v': 'obscure.v.05',\n",
    "        'obscured.a': 'obscure.n.01'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni di utility per il preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and punctuation from a given sentence\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = []\n",
    "    with open('/Users/jak/Documents/Uni/TLN/TLN/Radicioni/data/stop_words_FULL.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            stop_words.append(line.strip())\n",
    "    sentence = [w for w in sentence if w not in stop_words]\n",
    "    return sentence\n",
    "\n",
    "# Remove punctuation from a given list of words\n",
    "def remove_punctuation(list):\n",
    "    final_list = []\n",
    "    punct = set(['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '``', \"''\", '...', '’', '“', '”']) # Keeping '-' between words\n",
    "    for word in list:\n",
    "        for c in word:\n",
    "            if c in punct:\n",
    "                word = word.replace(c, '')\n",
    "            if c == '_':\n",
    "                word = word.replace(c, ' ')\n",
    "        final_list.append(word)\n",
    "    return final_list\n",
    "\n",
    "# Lemmatize a given list of words\n",
    "def lemmatize(list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_set = set()\n",
    "    for word in list:\n",
    "        final_set.add(lemmatizer.lemmatize(word))\n",
    "    return final_set\n",
    "\n",
    "# Full preprocessing of a given list of words\n",
    "def preprocess(list):\n",
    "    list = remove_stopwords(list) # Don't get why if I remove stopwords even here I get less words\n",
    "    list = remove_punctuation(list)\n",
    "    list = lemmatize(list)\n",
    "    list = remove_stopwords(list)\n",
    "    list = [x for x in list if x] # Remove empty strings\n",
    "    return set(list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del contesto dei sensi\n",
    "\n",
    "Creo una lista di content words per un dato synset, cercando all'interno di:\n",
    "- definizione del synset\n",
    "- esempi del synset\n",
    "- definizioni di iponimi e iperonimi\n",
    "- esempi di iponimi e iperonimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eclipse', 'total', 'action', 'celestial', 'activity', 'completely', 'body', 'obscures', 'abrupt', 'eclipsed', 'occurrence', 'obscured', 'player', 'light', 'sun', 'place', 'annoying', 'telephone', 'solar', 'ongoing', 'interrupt', 'moon', 'shining', 'hurt', 'lunar', 'partial', 'partially', 'interruption', 'earth', 'break'}\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# Get context for a given synset\n",
    "def get_context(synset):\n",
    "    context = set(synset.name().split('.')[0].split('_')) \n",
    "    context.update(synset.definition().lower().split())\n",
    "    for example in synset.examples():\n",
    "        context.update(example.lower().split())\n",
    "    for hypo in synset.hyponyms():\n",
    "        context.update(hypo.name().split('.')[0].split('_'))\n",
    "        context.update(hypo.definition().lower().split())\n",
    "        for example in hypo.examples():\n",
    "            context.update(example.lower().split())\n",
    "    for hyper in synset.hypernyms():\n",
    "        context.update(hyper.name().split('.')[0].split('_'))\n",
    "        context.update(hyper.definition().lower().split())\n",
    "        for example in hyper.examples():\n",
    "            context.update(example.lower().split())\n",
    "    context = preprocess(context)\n",
    "    return context\n",
    "\n",
    "context = get_context(wn.synset('eclipse.n.01'))\n",
    "print(context)\n",
    "print(len(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estraggo il contesto dei sensi per ogni synset di una parola data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['eclipse', 'total', 'action', 'celestial', 'activity', 'completely', 'body', 'obscures', 'abrupt', 'eclipsed', 'occurrence', 'obscured', 'player', 'light', 'sun', 'place', 'annoying', 'telephone', 'solar', 'ongoing', 'interrupt', 'moon', 'shining', 'hurt', 'lunar', 'partial', 'partially', 'interruption', 'earth', 'break'], ['overshadowed', 'happiness', 'threatening', 'day', 'dark', 'couple', 'greater', 'vision', 'brood', 'significance', 'long', 'overshadow', 'terrible', 'menacing', 'hang', 'tragedy', 'brooded'], ['house', 'intervention', 'overshadowed', 'eclipse', 'moon', 'today', 'planet', 'shadow', 'tree', 'celestial', 'tall', 'cast', 'star', 'overshadow', 'occulted', 'body', 'sun']]\n"
     ]
    }
   ],
   "source": [
    "# TODO: da cambiare di sihuro deh\n",
    "# Get context for all synsets of a given word\n",
    "def get_context_all(word):\n",
    "    context = []\n",
    "    for synset in wn.synsets(word):\n",
    "        context.append(get_context(synset))\n",
    "    return context\n",
    "\n",
    "context = get_context_all('eclipse')\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del contesto del frame\n",
    "\n",
    "Creo una lista di content words per un dato frame, cercando all'interno di:\n",
    "- Frame name\n",
    "- Frame definition\n",
    "- Frame Element name\n",
    "- Frame definition\n",
    "- Lexical Unit name\n",
    "- Lexical Unit definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'starting-point', 'woman', 'transport', 'brandhoek', 'embarked', 'expression', 'entrain', 'describes', 'bus', 'flight', 'ahead', 'boarding', 'cotheme', 'hellespont', 'main', 'shuttle', 'mounted', 'area', 'hour', 'drunk', '0730', '19', 'airborne', 'embarkation', 'bank', 'passenger', 'delayed', 'mount', 'embarkment', 'brave', 'hitting', 'time', 'describe', 'animal', 'order', 'property', 'lawler', 'vessel', 'southward', 'duration of final state', 'cod', 'rhine', 'co-traveller', 'boarded', 'independent', 'embarking', 'handsome', 'moving', 'place', 'merchant', 'mile', 'battalion', '1834', 'gather', 'station', 'embussed', 'headed', 'event', 'december', 'ride', 'transportation', 'kate', 'horse', 'participant', 'power', 'glittered', 'path', 'aboard', 'looked', 'board', '5th', 'element', 'aircraft', 'circling', 'fn', 'jump', 'circumstance', 'entrained', 'bicycle', 'conveyance', 'sun', 'reportedly', 'bart', 'plane', 'october', 'depictive', 'motion', 'bar', 'board vehicle', 'embark', 'armour', 'location', 'express', 'england', 'america', 'train', 'unit', 'emplaned', '1st', 'september', 'entity', 'implies', 'therese', 'training', 'frame', 'traveller', 'army', 'manner', 'intend', 'amount', 'child', 'ship', 'source', 'embus', 'purpose', 'driver', 'vehicle', 'spends', 'relation', 'hop', '9am', 'crew', 'gaggle', 'swordfish', 'definite', 'theme', '4', 'trajectory', '40', 'intended', 'emplane', 'enter'}\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "# Get context for a given frame\n",
    "def get_frame_context(frame):\n",
    "    context = set(frame.definition.lower().split())\n",
    "    context.update(frame.name.lower().split())\n",
    "    for fe in frame.FE:\n",
    "        context.update(frame.FE[fe].definition.lower().split())\n",
    "        context.update(fe.lower().split())\n",
    "    for lu in frame.lexUnit:\n",
    "        context.update(frame.lexUnit[lu].lexemes[0].name.lower().split())\n",
    "        context.update(frame.lexUnit[lu].definition.lower().split())\n",
    "    context = preprocess(context)\n",
    "    return context\n",
    "\n",
    "context = get_frame_context(fn.frame('Board_vehicle'))\n",
    "print(context)\n",
    "print(len(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del contesto del frame element\n",
    "\n",
    "Creo una lista di content words per un dato frame element, cercando all'interno di:\n",
    "- Frame Element name\n",
    "- Frame Element definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context for a given frame element\n",
    "def get_fe_context(frame_element):\n",
    "    context = set(frame_element.definition.lower().split())\n",
    "    context.update(frame_element.name.lower().split())\n",
    "    context = preprocess(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione del contesto del lexical unit\n",
    "\n",
    "Creo una lista di content words per un dato lexical unit, cercando all'interno di:\n",
    "- Lexical Unit name\n",
    "- Lexical Unit definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context for a given lexical unit\n",
    "def get_lu_context(lexical_unit):\n",
    "    context = set(lexical_unit.lexemes[0].name.lower().split())\n",
    "    context.update(lexical_unit.definition.lower().split())\n",
    "    context = preprocess(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping dei synset con i frame\n",
    "\n",
    "Approccio bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best synset for a given word and context\n",
    "def find_best_syn(word, context):\n",
    "    best = 0\n",
    "    best_syn = None\n",
    "    for synset in wn.synsets(word):\n",
    "        syn_context = get_context(synset)\n",
    "        score = len(syn_context.intersection(context)) + 1 # + 1 According to the paper\n",
    "        # print(synset, score)\n",
    "        if score > best:\n",
    "            best = score\n",
    "            best_syn = synset\n",
    "    return best_syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disambiguazione dei synset di: Frames, FEs e LUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRAME: Suicide_attack - Suicide - Synset('suicide.n.01')\n",
      "FE: Victim - Synset('victim.n.01')\n",
      "FE: Weapon - Synset('weapon.n.01')\n",
      "FE: Assailant - Synset('attacker.n.01')\n",
      "FE: Manner - Synset('manner.n.01')\n",
      "FE: Means - Synset('means.n.01')\n",
      "FE: Place - Synset('topographic_point.n.01')\n",
      "FE: Purpose - Synset('purpose.n.01')\n",
      "FE: Time - Synset('time.n.01')\n",
      "FE: Source - Synset('beginning.n.04')\n",
      "FE: Depictive - Synset('delineative.s.01')\n",
      "FE: Re-encoding - None\n",
      "FE: Result - Synset('consequence.n.01')\n",
      "FE: Containing_event - None\n",
      "FE: Explanation - Synset('explanation.n.01')\n",
      "FE: Particular_iteration - None\n",
      "FE: Circumstances - Synset('circumstance.n.01')\n",
      "FE: Path - Synset('path.n.03')\n",
      "LU: suicide attack.n - Synset('suicide.n.01')\n",
      "LU: suicide bombing.n - Synset('suicide.n.02')\n",
      "LU: kamikaze [attack].n - Synset('kamikaze.n.01')\n",
      "LU: suicide bomber.n - Synset('suicide.n.01')\n",
      "LU: suicide attacker.n - Synset('suicide.n.02')\n",
      "LU: kamikaze [person].n - Synset('kamikaze.n.01')\n",
      "Total FEs + LUs + frame name: 24\n",
      "----------------------------------------------------------------------\n",
      "FRAME: Board_vehicle - Vehicle - Synset('vehicle.n.01')\n",
      "FE: Traveller - Synset('traveler.n.01')\n",
      "FE: Vehicle - Synset('vehicle.n.01')\n",
      "FE: Source - Synset('generator.n.03')\n",
      "FE: Manner - Synset('manner.n.01')\n",
      "FE: Path - Synset('way.n.05')\n",
      "FE: Duration_of_final_state - None\n",
      "FE: Depictive - Synset('delineative.s.01')\n",
      "FE: Time - Synset('time.n.01')\n",
      "FE: Cotheme - None\n",
      "FE: Purpose - Synset('purpose.n.01')\n",
      "FE: Place - Synset('put.v.01')\n",
      "FE: Circumstances - Synset('circumstance.n.01')\n",
      "LU: board.v - Synset('board.v.01')\n",
      "LU: get.v - Synset('get.v.01')\n",
      "LU: embark.v - Synset('embark.v.01')\n",
      "LU: emplane.v - Synset('emplane.v.01')\n",
      "LU: entrain.v - Synset('entrain.v.01')\n",
      "LU: mount.v - Synset('mount.n.04')\n",
      "LU: hop.v - Synset('hop.n.01')\n",
      "LU: embarkation.n - Synset('boarding.n.01')\n",
      "LU: embarkment.n - Synset('boarding.n.01')\n",
      "LU: embus.v - None\n",
      "Total FEs + LUs + frame name: 23\n",
      "----------------------------------------------------------------------\n",
      "FRAME: Simple_name - Name - Synset('name.n.01')\n",
      "FE: Term - Synset('term.n.01')\n",
      "FE: Entity - Synset('entity.n.01')\n",
      "FE: Speaker - Synset('speaker.n.01')\n",
      "LU: term.n - Synset('term.n.01')\n",
      "LU: word.n - Synset('word.n.01')\n",
      "Total FEs + LUs + frame name: 6\n",
      "----------------------------------------------------------------------\n",
      "FRAME: Access_scenario - Access - Synset('access.n.03')\n",
      "FE: Theme - Synset('subject.n.01')\n",
      "FE: Useful_location - None\n",
      "FE: Barrier - Synset('barrier.n.01')\n",
      "Total FEs + LUs + frame name: 4\n",
      "----------------------------------------------------------------------\n",
      "FRAME: Eclipse - Eclipse - Synset('eclipse.n.01')\n",
      "FE: Obstruction - Synset('obstruction.n.01')\n",
      "FE: Eclipsed - Synset('overshadow.v.01')\n",
      "FE: Vantage_point - Synset('vantage_point.n.01')\n",
      "FE: Degree - Synset('degree.n.02')\n",
      "FE: Subregion - None\n",
      "LU: hide.v - Synset('hide.v.01')\n",
      "LU: eclipse.v - Synset('eclipse.n.01')\n",
      "LU: mask.v - Synset('mask.n.01')\n",
      "LU: cover.v - Synset('cover.v.01')\n",
      "LU: obscure.v - Synset('obscure.v.01')\n",
      "LU: screen.v - Synset('screen.n.04')\n",
      "LU: veil.v - Synset('veil.v.01')\n",
      "LU: hidden.a - Synset('hide.v.01')\n",
      "LU: blot out.v - Synset('smudge.n.02')\n",
      "LU: obscured.a - Synset('obscure.v.01')\n",
      "Total FEs + LUs + frame name: 16\n",
      "----------------------------------------------------------------------\n",
      "Total FEs + LUs + frame name: 73\n"
     ]
    }
   ],
   "source": [
    "# Find best synset for:\n",
    "# - frame (with the simplified name)\n",
    "#   - frame elements\n",
    "#   - lexical units\n",
    "def disambiguate():\n",
    "    total = 0\n",
    "    dict = {}\n",
    "    for frame_id in frame_ids:\n",
    "        frame = fn.frame_by_id(frame_id)\n",
    "        dict[frame.name] = {}\n",
    "\n",
    "        # Frame\n",
    "        word = simplified_names[frame_ids.index(frame_id)]\n",
    "        frame_context = get_frame_context(frame)\n",
    "        best_syn = find_best_syn(word, frame_context)\n",
    "        dict[frame.name][word] = best_syn\n",
    "        print(f'FRAME: {frame.name} - {word} - {best_syn}')\n",
    "        \n",
    "\n",
    "        # Frame elements\n",
    "        for fe in frame.FE:\n",
    "            fe_context = get_fe_context(frame.FE[fe])\n",
    "            best_syn = find_best_syn(fe, fe_context)\n",
    "            dict[frame.name][fe] = best_syn\n",
    "            print(f'FE: {fe} - {best_syn}')\n",
    "            \n",
    "\n",
    "        # Lexical units\n",
    "        count = 0\n",
    "        for lu in frame.lexUnit:\n",
    "            # Disambiguate only the first 10 lexical units -- too many to manually annotate\n",
    "            if count >= 10:\n",
    "                break\n",
    "            count += 1\n",
    "            lu_context = get_lu_context(frame.lexUnit[lu])\n",
    "            best_syn = find_best_syn(frame.lexUnit[lu].lexemes[0].name, lu_context)\n",
    "            dict[frame.name][lu] = best_syn\n",
    "            print(f'LU: {lu} - {best_syn}')\n",
    "        \n",
    "        # number of FEs + LUs\n",
    "        print(f'Total FEs + LUs + frame name: {len(frame.FE) + count + 1}')\n",
    "        # Separator\n",
    "        print('-' * 70)\n",
    "        total += len(frame.FE) + count + 1\n",
    "\n",
    "\n",
    "    print(f'Total FEs + LUs + frame name: {total}')\n",
    "    return dict\n",
    "\n",
    "result = disambiguate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutare i risultati con le annotations manuali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.27777777777779%\n",
      "Correct: 47\n",
      "Total: 72\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of the disambiguation using the manually annotated data\n",
    "def compute_accuracy():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for key in result:\n",
    "        # print('-' * 70)\n",
    "        for word in result[key]:\n",
    "            if word in annotations[key]:\n",
    "                total += 1\n",
    "                # print(f'{total}     {word} - {result[key][word]} - {annotations[key][word]}')\n",
    "                if result[key][word] is None and annotations[key][word] is None:\n",
    "                    correct += 1\n",
    "                elif result[key][word] is not None:\n",
    "                    if result[key][word].name() == annotations[key][word]:\n",
    "                        correct += 1\n",
    "    return correct, total\n",
    "\n",
    "correct, total = compute_accuracy()\n",
    "print(f'Accuracy: {(correct / total) * 100}%')\n",
    "print(f'Correct: {correct}')\n",
    "print(f'Total: {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in a csv file\n",
    "def save_result():\n",
    "    with open('result.csv', 'w') as f:\n",
    "        for key in result:\n",
    "            cap_title = key.upper()\n",
    "            f.write(f'{cap_title}\\n')\n",
    "            for word in result[key]:\n",
    "                cap_word = word.upper()\n",
    "                if result[key][word] is not None:\n",
    "                    synset = result[key][word].name()\n",
    "                else:\n",
    "                    synset = 'None'\n",
    "                f.write(f' - {cap_word}\\t Found: {synset}\\t Gold: {annotations[key][word]}\\n')\n",
    "         \n",
    "save_result()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5587bf738ccec4bb524b17f914a1c07eb93a384af8fabdf9504251bc61fb6171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
